{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import pydicom\n",
    "import cv2\n",
    "import json\n",
    "import pandas as pd\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "from PIL import Image\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import LinearRing\n",
    "from skimage import draw\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.layers import Input,Conv2D, MaxPooling2D, UpSampling2D, Dropout, Cropping2D, concatenate, Activation,Conv2DTranspose\n",
    "from tensorflow.keras.layers import BatchNormalization, SeparableConv2D, Add, MaxPool2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, History, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_loading(path,image_size = 512):\n",
    "#     mkfolder(path)\n",
    "    trainlist = glob.glob(path+'/*.dcm')\n",
    "\n",
    "# 이미 한번 돌려서 저장해논거 다시 저장x 하기 위해 주석처리        \n",
    "#     data_pre(trainlist)\n",
    "\n",
    "    img_mask_path = '/home/hackerton/jupyter/Spine_data/A_img_mask/'\n",
    "    \n",
    "    train_img_path = img_mask_path +'/*.jpg'\n",
    "    train_mask_path = img_mask_path +'/*.png'\n",
    "\n",
    "# 이미 한번 돌려서 아래 주소로 대체\n",
    "#     aug_path = augmentation(train_img_path,train_mask_path)\n",
    "    \n",
    "    aug_path = '/home/hackerton/jupyter/Spine_data/A_aug/'\n",
    "\n",
    "    imgs_train, imgs_mask_train, imgs_name = create_train_data(aug_path, image_size, image_size, 'train', 'jpg')\n",
    "    return imgs_train, imgs_mask_train, imgs_name\n",
    "\n",
    "def data_pre(dcmlist):\n",
    "# 이미 한번 돌려서 저장해논거 다시 저장x 하기 위해 주석처리    \n",
    "    img_mask_path = '/home/hackerton/jupyter/Spine_data/A_img_mask/'\n",
    "    mkfolder(img_mask_path)\n",
    "    \n",
    "    for i in range(len(dcmlist)):\n",
    "        # try:\n",
    "        if i != 133:   \n",
    "            file_name = dcmlist[i].split('/')[-1].split('.')[0]\n",
    "            dicompath = dcmlist[i]\n",
    "            dicom = pydicom.read_file(dicompath)\n",
    "            img = dicom.pixel_array\n",
    "            img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)\n",
    "            # except:\n",
    "            #     print(dicompath)\n",
    "            #     continue\n",
    "            pad_img = zero_padding(img)\n",
    "            image = Image.fromarray(pad_img)\n",
    "            image = image.convert('L')\n",
    "            file_save = file_name + '.jpg'\n",
    "            print(file_save)\n",
    "\n",
    "            image.save(img_mask_path + file_save)\n",
    "    print('Complete !')\n",
    "    \n",
    "    for dcm in dcmlist:\n",
    "        jsonfile = dcm[:-4]+'.json'\n",
    "        \n",
    "        file_name = dcm.split('/')[-1].split('.')[0]\n",
    "        \n",
    "        reader = sitk.ReadImage(dcm)\n",
    "        image_array = sitk.GetArrayFromImage(reader)\n",
    "        height = reader.GetMetaData('0028|0010')\n",
    "        width = reader.GetMetaData('0028|0011')\n",
    "        data = []\n",
    "        for line in open(jsonfile,'r'):\n",
    "            if dcm !=  dcmlist[133]:\n",
    "                data.append(json.loads(line))\n",
    "        for json_data in data:\n",
    "            mask = np.zeros((int(height), int(width)))\n",
    "            if json_data['annotation']['ANNOTATION_DATA'] is not None:\n",
    "                for m in json_data['annotation']['ANNOTATION_DATA']:\n",
    "                    if 'm_points' in m:\n",
    "                        a = []\n",
    "                        for i in m['m_points']:\n",
    "                            b = (i['x'], i['y'])\n",
    "                            a.append(b)\n",
    "                        r = LinearRing(a)\n",
    "                        s = Polygon(r)\n",
    "                        x, y = s.exterior.coords.xy\n",
    "                        maskd = poly2mask(y, x, (int(height), int(width)))\n",
    "                        mask = mask + maskd\n",
    "                mask = mask*255\n",
    "                mask = zero_padding(mask)\n",
    "                mask = np.expand_dims(mask, axis=0)\n",
    "                img = sitk.GetImageFromArray(mask.astype('uint8'))\n",
    "                num = 0\n",
    "                maskpath = img_mask_path + file_name +'.png'\n",
    "                print(maskpath)\n",
    "                sitk.WriteImage(img, maskpath)\n",
    "            else:\n",
    "                print('haha')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(img_path,mask_path):\n",
    "    img_li = sorted(glob.glob(img_path))\n",
    "    mask_li = sorted(glob.glob(mask_path))\n",
    "    print(len(img_li), len(mask_li))\n",
    "    i=0\n",
    "\n",
    "    for img, mask in zip(img_li, mask_li):\n",
    "        if i%100==0:\n",
    "            print('{}/{}'.format(i, len(img_li)))\n",
    "        savepath = '/home/hackerton/jupyter/Spine_data/A_aug/'\n",
    "        mkfolder(savepath)\n",
    "        ori_img = cv2.imread(img, 0)\n",
    "        mask_img = cv2.imread(mask, 0)\n",
    "        img_name = img[img.rindex('/')+1:-4]\n",
    "        mask_name = img[mask.rindex('/')+1:-4]\n",
    "        print(img_name)\n",
    "        print(mask_name)\n",
    "        cv2.imwrite(savepath+'/{}.jpg'.format(img_name), cv2.resize(ori_img, (512,512)))\n",
    "        cv2.imwrite(savepath+'/{}.png'.format(img_name), cv2.resize(mask_img, (512,512)))\n",
    "        for j in range(9):\n",
    "            aug_img, aug_mask = Augment_crop(ori_img, mask_img)        \n",
    "            aug_img = cv2.resize(aug_img, (512,512))\n",
    "            aug_mask = cv2.resize(aug_mask, (512,512))\n",
    "            cv2.imwrite(savepath+'/{}_{}.jpg'.format(img_name, j), aug_img)\n",
    "            cv2.imwrite(savepath+'/{}_{}.png'.format(img_name, j), aug_mask)\n",
    "        i+=1\n",
    "    return savepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data(train_path, out_rows, out_cols, name, img_type):\n",
    "    print('-'*30)\n",
    "    print('Creating training images...')\n",
    "    print('-'*30)\n",
    "    \n",
    "    imgs = glob.glob(train_path +\"*.\" + img_type)\n",
    "    imgdatas = np.ndarray((len(imgs),out_rows,out_cols,1), dtype=np.uint8)\n",
    "    imglabels = np.ndarray((len(imgs),out_rows,out_cols,1), dtype=np.uint8)\n",
    "    imgnames=[]\n",
    "    for i, imgname in enumerate(imgs):\n",
    "        if i%1000==0:\n",
    "            print('{}/{}'.format(i, len(imgs)))\n",
    "        midname = imgname[imgname.rindex(\"/\")+1:-4]     \n",
    "        img = load_img(imgname, color_mode = \"grayscale\")\n",
    "        label = load_img(imgname.replace('jpg', 'png'), color_mode = \"grayscale\")\n",
    "        img=img.resize((out_rows,out_cols))\n",
    "        label=label.resize((out_rows,out_cols))\n",
    "\n",
    "        img = img_to_array(img)\n",
    "        label = img_to_array(label)\n",
    "        imgdatas[i] = img\n",
    "        imglabels[i] = label\n",
    "        imgnames.append(midname)\n",
    "        \n",
    "    imgdatas = imgdatas.astype('float32')\n",
    "    imglabels = imglabels.astype('float32')\n",
    "    \n",
    "    print('img : ', imgdatas.max())\n",
    "    print('mask : ',imglabels.max())\n",
    "    \n",
    "    print('-'*30)\n",
    "    print('normalization start...')\n",
    "    print('-'*30)\n",
    "    imgdatas = imgdatas/255.0\n",
    "    \n",
    "    imglabels[imglabels <= 127] = 0\n",
    "    imglabels[imglabels > 127] = 1\n",
    "    \n",
    "    \n",
    "    print('img : ',imgdatas.max())\n",
    "    print('mask : ',imglabels.max())\n",
    "    print('mask : ',imglabels.min())\n",
    "    \n",
    "    print('loading done')\n",
    "    return(imgdatas,imglabels,imgnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(img):\n",
    "    y, x = img.shape\n",
    "    if x > y:\n",
    "        new_size = x\n",
    "        add_size = x-y\n",
    "        \n",
    "        add_image = np.zeros((new_size, add_size))\n",
    "        if add_size % 2==0:\n",
    "            add_img = np.zeros((int(add_size/2), new_size))\n",
    "            img = np.concatenate((add_img,img, add_img),axis=0)\n",
    "\n",
    "        elif add_size % 2==1:\n",
    "            add_img_top = np.zeros((int(add_size/2),new_size))\n",
    "            add_img_bot = np.zeros((int(add_size/2)+1, new_size))\n",
    "            img = np.concatenate((add_img_top,img, add_img_bot),axis=0)\n",
    "\n",
    "    elif y > x:\n",
    "        new_size = y\n",
    "        add_size = y-x\n",
    "        \n",
    "        add_image = np.zeros((add_size, new_size))\n",
    "        if add_size % 2==0:\n",
    "            add_img = np.zeros((new_size, int(add_size/2)))\n",
    "            img = np.concatenate((add_img,img, add_img),axis=1)\n",
    "\n",
    "        elif add_size % 2==1:\n",
    "            add_img_left = np.zeros((new_size, int(add_size/2)))\n",
    "            add_img_right = np.zeros((new_size, int(add_size/2)+1))\n",
    "            img = np.concatenate((add_img_left,img, add_img_right),axis=1)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def poly2mask(vertex_row_coords, vertex_col_coords, shape):\n",
    "    fill_row_coords, fill_col_coords = draw.polygon(\n",
    "        vertex_row_coords, vertex_col_coords, shape)\n",
    "    mask = np.zeros(shape, dtype=np.bool)\n",
    "    mask[fill_row_coords, fill_col_coords] = True\n",
    "    return mask\n",
    "\n",
    "def mkfolder(folder):\n",
    "    if not os.path.lexists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "def weight_center(img):\n",
    "    contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "    cnt = contours[0] \n",
    "    mmt = cv2.moments(cnt) \n",
    "    cx = int(mmt['m10']/mmt['m00']) \n",
    "    cy = int(mmt['m01']/mmt['m00']) \n",
    "    print( 'x 무게중심', cx, 'y 무게중심', cy )\n",
    "    return cx, cy\n",
    "def randomRoate(img, label, p, angle_range):\n",
    "    if len(img.shape)==3:\n",
    "        height, width, channel = img.shape\n",
    "    else:\n",
    "        height, width = img.shape\n",
    "    \n",
    "    random_angle = np.random.randint(-angle_range/2, angle_range/2)*2\n",
    "    matrix = cv2.getRotationMatrix2D((width/2, height/2), random_angle, 1)\n",
    "    rotate_img = cv2.warpAffine(img, matrix, (width, height))\n",
    "    rotate_label = cv2.warpAffine(label, matrix, (width, height))\n",
    "    \n",
    "    return rotate_img, rotate_label\n",
    "\n",
    "def find_top_point(ori_img, mask_img):\n",
    "    mask_img_T = mask_img.T\n",
    "\n",
    "    tmp_index=[]\n",
    "    for i, c in enumerate(mask_img_T):\n",
    "        if len(np.unique(c))>1:\n",
    "            tmp_index.append(i)\n",
    "    x_min = min(tmp_index)\n",
    "    x_max = max(tmp_index)\n",
    "\n",
    "    p_x = int((x_min+x_max)/2) \n",
    "    p_y = np.where(mask_img_T[int((x_min+x_max)/2)]==255)[0][0]\n",
    "    p = (p_x, p_y)\n",
    "\n",
    "    return p_x, p_y\n",
    "\n",
    "def Augment_crop(img, mask):\n",
    "    p_x, p_y=find_top_point(img, mask)\n",
    "    rotate_img, rotate_mask = randomRoate(img, mask, (p_x, p_y), 20)\n",
    "    random_size = np.random.randint(15,35)*20 # 300-600\n",
    "    \n",
    "    h, w= img.shape\n",
    "    x1 = p_x-random_size if p_x-random_size>0 else 0\n",
    "    x2 = p_x+random_size if p_x+random_size<w else w\n",
    "    y1 = p_y-random_size if p_y-random_size>0 else 0\n",
    "    y2 = p_y+random_size if p_y+random_size<h else h\n",
    "    \n",
    "    crop_img = rotate_img[y1:y2,x1:x2]\n",
    "    crop_mask = rotate_mask[y1:y2,x1:x2]\n",
    "\n",
    "    return crop_img, crop_mask\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        return (2. * intersection + K.epsilon()) / (K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon())\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "        return 1-dice_coef(y_true, y_pred)\n",
    "\n",
    "def sens(y_true, y_pred): # sensitivity, recall\n",
    "    print(y_pred)\n",
    "    print(y_true)\n",
    "    y_target_yn = K.round(K.clip(y_true, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "def sch(epoch):\n",
    "    if epoch>100 and epoch<=250:\n",
    "        return 0.0001\n",
    "    elif epoch>250:\n",
    "        return 0.00001\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def hourglass_network(input_shape, num_classes, num_stacks, num_channels):\n",
    "    \n",
    "    input = Input(input_shape)\n",
    "\n",
    "    start = front_module(input, num_channels)\n",
    "\n",
    "    head_next_stage = start\n",
    "\n",
    "    outs = []\n",
    "    for i in range(num_stacks):\n",
    "        head_next_stage, head_to_loss = hourglass_module(head_next_stage, num_classes, num_channels, i)\n",
    "        outs.append(head_to_loss)\n",
    "    \n",
    "    x = BatchNormalization()(outs[-1])\n",
    "    x = Conv2DTranspose(int(num_classes/2), (2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2DTranspose(int(num_classes/4), (2, 2), strides=(2, 2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(1, (1, 1), activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=input, outputs=x)\n",
    "\n",
    "    return model\n",
    "\n",
    "def hourglass_module(bottom, num_classes, num_channels, hgid):\n",
    "\n",
    "    left_features = left_half_blocks(bottom, hgid, num_channels)\n",
    "\n",
    "    rf1 = right_half_blocks(left_features, hgid, num_channels)\n",
    "    \n",
    "    head_next_stage, head_parts = heads(bottom, rf1, num_classes, hgid, num_channels)\n",
    "\n",
    "    return head_next_stage, head_parts\n",
    "\n",
    "def front_module(input, num_channels):\n",
    "    \n",
    "    x = Conv2D(64,kernel_size=(7, 7), strides=(2, 2), padding='same', activation='relu', name='front_conv_1x1_x1')(\n",
    "        input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = bottleneck_block(x, int(num_channels // 2), 'front_residual_x1')\n",
    "    x = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    x = bottleneck_block(x, int(num_channels // 2), 'front_residual_x2')\n",
    "    x = bottleneck_block(x, num_channels, 'front_residual_x3')\n",
    "\n",
    "    return x\n",
    "\n",
    "def heads(prelayerfeatures, rf1, num_classes, hgid, num_channels):\n",
    "    \n",
    "    head = Conv2D(num_channels, kernel_size=(1, 1), activation='relu', padding='same', name=str(hgid) + 'conv_1x1_x1')(\n",
    "        rf1)\n",
    "    head = BatchNormalization()(head)\n",
    "\n",
    "    head_parts = Conv2D(num_classes, kernel_size=(1, 1), activation='linear', padding='same',\n",
    "                        name=str(hgid) + 'conv_1x1_parts')(head)\n",
    "\n",
    "    head = Conv2D(num_channels, kernel_size=(1, 1), activation='linear', padding='same',\n",
    "                  name=str(hgid) + 'conv_1x1_x2')(head)\n",
    "    head_m = Conv2D(num_channels, kernel_size=(1, 1), activation='linear', padding='same',\n",
    "                    name=str(hgid) + 'conv_1x1_x3')(head_parts)\n",
    "\n",
    "    head_next_stage = Add()([head, head_m, prelayerfeatures])\n",
    "    return head_next_stage, head_parts\n",
    "\n",
    "\n",
    "def bottleneck_block(bottom, num_out_channels, block_name):\n",
    "\n",
    "    if K.int_shape(bottom)[-1] == num_out_channels:\n",
    "        skip = bottom\n",
    "    else:\n",
    "        skip = Conv2D(num_out_channels, kernel_size=(1, 1), activation='relu', padding='same',\n",
    "                       name=block_name + 'skip')(bottom)\n",
    "\n",
    "    x = Conv2D(int(num_out_channels / 2), kernel_size=(1, 1), activation='relu', padding='same',\n",
    "                name=block_name + 'conv_1x1_x1')(bottom)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(int(num_out_channels / 2), kernel_size=(3, 3), activation='relu', padding='same',\n",
    "                name=block_name + 'conv_3x3_x2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(num_out_channels, kernel_size=(1, 1), activation='relu', padding='same',\n",
    "                name=block_name + 'conv_1x1_x3')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add(name=block_name + 'residual')([skip, x])\n",
    "\n",
    "    return x\n",
    "\n",
    "def left_half_blocks(bottom, hglayer, num_channels):\n",
    "\n",
    "    hgname = 'hg' + str(hglayer)\n",
    "\n",
    "    f1 = bottleneck_block(bottom, num_channels, hgname + 'l1')\n",
    "    x = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(f1)\n",
    "\n",
    "    f2 = bottleneck_block(x, num_channels, hgname + 'l2')\n",
    "    x = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(f2)\n",
    "\n",
    "    f4 = bottleneck_block(x, num_channels, hgname + 'l4')\n",
    "    x = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(f4)\n",
    "\n",
    "    f8 = bottleneck_block(x, num_channels, hgname + 'l8')\n",
    "\n",
    "    return (f1, f2, f4, f8)\n",
    "\n",
    "def left_to_right(left, right, name, num_channels):\n",
    "    \n",
    "    xleft = bottleneck_block(left, num_channels, name + 'connect')\n",
    "    xright = UpSampling2D()(right)\n",
    "    add = Add()([xleft, xright])\n",
    "    out = bottleneck_block(add, num_channels, name + 'connect_conv')\n",
    "    return out\n",
    "\n",
    "def bottom_layer(lf8, hgid, num_channels):\n",
    "    \n",
    "    lf8_connect = bottleneck_block(lf8, num_channels, str(hgid) + \"lf8\")\n",
    "\n",
    "    x = bottleneck_block(lf8, num_channels, str(hgid) + \"lf8_x1\")\n",
    "    x = bottleneck_block(x, num_channels, str(hgid) + \"lf8_x2\")\n",
    "    x = bottleneck_block(x, num_channels, str(hgid) + \"lf8_x3\")\n",
    "\n",
    "    rf8 = Add()([x, lf8_connect])\n",
    "\n",
    "    return rf8\n",
    "\n",
    "def right_half_blocks(leftfeatures, hglayer, num_channels):  #branch_add\n",
    "    lf1, lf2, lf4, lf8 = leftfeatures\n",
    "\n",
    "    rf8 = bottom_layer(lf8, hglayer, num_channels)\n",
    "\n",
    "    rf4 = left_to_right(lf4, rf8, 'hg' + str(hglayer) + 'rf4', num_channels)\n",
    "\n",
    "    rf2 = left_to_right(lf2, rf4, 'hg' + str(hglayer) + 'rf2', num_channels)\n",
    "\n",
    "    rf1 = left_to_right(lf1, rf2, 'hg' + str(hglayer) + 'rf1', num_channels)\n",
    "\n",
    "    return rf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingWarmup(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, epochs_per_cycle,iteration, max_lr, min_lr, verbose = 1):\n",
    "        self.epochs_per_cycle = epochs_per_cycle\n",
    "        self.max_lr = max_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.iteration = iteration;\n",
    "        self.steps = 0;\n",
    "        self.learning_rate = max_lr;\n",
    "        self.epochs = 0; # epoch to search min_lr for each iteration \n",
    "        self.warmup_epoch = 10  # warmup epcch\n",
    "        self.verbose = verbose # log\n",
    "        self.lrates = list() # for graph\n",
    "        \n",
    "        \n",
    "    def cosine_annealing(self, epoch, epochs_per_cycle, max_lr):\n",
    "        self.epochs += 1; \n",
    "        cos_inner = (math.pi * (self.epochs % epochs_per_cycle)) / (epochs_per_cycle)\n",
    "        self.learning_rate = max_lr/2 * (math.cos(cos_inner) + 1)\n",
    "        \n",
    "        if ((self.epochs % epochs_per_cycle) == (epochs_per_cycle-1)):\n",
    "            self.steps += 1\n",
    "            self.max_lr *= 0.8\n",
    "            self.epochs = 0;\n",
    "            self.epochs_per_cycle = math.floor(self.epochs_per_cycle*1.2)\n",
    "            \n",
    "        return max_lr/2 * (math.cos(cos_inner) + 1)\n",
    "  \n",
    "    def warm_up(self, epoch):\n",
    "        \n",
    "        self.learning_rate = self.max_lr * epoch / self.warmup_epoch\n",
    "        \n",
    "        return self.learning_rate\n",
    "\n",
    "    # calculate and set learning rate at the start of the epoch\n",
    "    def on_epoch_begin(self, epoch, logs = None):\n",
    "        if (epoch < self.warmup_epoch):\n",
    "            # warm up learning rate\n",
    "            lr = self.warm_up(epoch)\n",
    "       \n",
    "        elif(self.steps < self.iteration):\n",
    "            # calculate cosine learning rate\n",
    "            lr = self.cosine_annealing(epoch, self.epochs_per_cycle, self.max_lr)\n",
    "            \n",
    "        else:\n",
    "            lr = self.min_lr\n",
    "        \n",
    "        if (self.verbose == 1):\n",
    "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learng rate to %s.' % (epoch + 1, lr))  \n",
    "\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "        self.lrates.append(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep(imgs_train,imgs_mask_train,path,batch_size = 4,epochs = 10,image_size=512): \n",
    "    gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(gpus)\n",
    "    strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n",
    "    \n",
    "    with strategy.scope():\n",
    "        model = hourglass_network(input_shape=(image_size, image_size, 1), num_classes=16, num_stacks=1, num_channels=32)\n",
    "        model.compile(optimizer=Adam(lr=0.001), loss=dice_coef_loss, \n",
    "                        metrics=['accuracy', sens, dice_coef_loss])\n",
    "    \n",
    "    check_model_path = path+'A_check/'\n",
    "    predict_path = path+'A_pred/'\n",
    "    mkfolder(check_model_path)\n",
    "    mkfolder(predict_path)\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(check_model_path + 'final3_{epoch:d}_{loss:f}.hdf5', \n",
    "                                        monitor='val_dice_coef_loss',verbose=1, \n",
    "                                        save_best_only=False)\n",
    "    earlystopping = EarlyStopping(monitor='val_dice_coef_loss', patience=30, restore_best_weights=True)\n",
    "    cosine_schedule = CosineAnnealingWarmup(epochs_per_cycle=60, iteration=1,max_lr = 1e-3, min_lr = 1e-6)\n",
    "    \n",
    "    print('Fitting model...')\n",
    "    model.fit(imgs_train, imgs_mask_train, batch_size=batch_size, epochs=epochs, verbose=1, \n",
    "              validation_split=0.2, shuffle=True, callbacks=[model_checkpoint, cosine_schedule ,earlystopping])\n",
    "    print('save model')\n",
    "    model.save(predict_path + 'final3.h5')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_save(pred_list,name_list):\n",
    "    pred_img_path = '/home/hackerton/jupyter/Spine_data/A_pred/'\n",
    "    if not os.path.isdir(pred_img_path):\n",
    "        os.makedirs(pred_img_path)\n",
    "\n",
    "    imgs = pred_list\n",
    "    for i in range(imgs.shape[0]):\n",
    "        img = imgs[i]\n",
    "        img[img <= 0.5] = 0\n",
    "        img[img > 0.5] = 255\n",
    "        img = array_to_img(img)\n",
    "        img.save(pred_img_path+\"%s_pred.png\" %(name_list[i]))\n",
    "\n",
    "def predict_val(model,test_path,image_size=512):\n",
    "    test_list = glob.glob(test_path+'*.dcm')\n",
    "#     data_pre(test_list)\n",
    "    \n",
    "    img_mask_path ='/home/hackerton/jupyter/Spine_data/A_img_mask/'\n",
    "    \n",
    "    imgs_test, imgs_label_test, test_name = create_test_data(img_mask_path, image_size, image_size, 'test', 'jpg')\n",
    "    print('predict test data')\n",
    "    \n",
    "    imgs_label_pred = model.predict(imgs_test, batch_size=4, verbose=1)\n",
    "    name_list=test_name\n",
    "    df = pd.DataFrame(columns=['name', 'acc', 'sen', 'spe', 'dsc'],dtype = float)\n",
    "    df = df.astype({'name': 'str'})\n",
    "\n",
    "    true_list=imgs_label_test\n",
    "    print(true_list.shape)\n",
    "\n",
    "    pred_list=imgs_label_pred\n",
    "    print(np.unique(pred_list))\n",
    "    pred_list[pred_list > 0.5] = 1\n",
    "    pred_list[pred_list <= 0.5] = 0\n",
    "    \n",
    "    sensitivity=[]\n",
    "    specificity=[]\n",
    "    acc=[]\n",
    "    dsc=[]\n",
    "\n",
    "    for i in range(len(true_list)):\n",
    "        yt=true_list[i].flatten()\n",
    "        yp=pred_list[i].flatten()\n",
    "        mat=confusion_matrix(yt,yp)\n",
    "        if len(mat) == 2:\n",
    "            ac=(mat[1,1]+mat[0,0])/(mat[1,0]+mat[1,1]+mat[0,1]+mat[0,0])\n",
    "            st=mat[1,1]/(mat[1,0]+mat[1,1])\n",
    "            sp=mat[0,0]/(mat[0,1]+mat[0,0])\n",
    "            if mat[1,0]+mat[1,1] == 0:\n",
    "                specificity.append(sp)\n",
    "                acc.append(ac)\n",
    "            else:\n",
    "                sensitivity.append(st)  \n",
    "                specificity.append(sp)\n",
    "                acc.append(ac)\n",
    "        else:\n",
    "            specificity.append(1)\n",
    "            acc.append(1)\n",
    "\n",
    "        yt=true_list[i]\n",
    "        yp=pred_list[i]\n",
    "        if np.sum(yt) != 0 and np.sum(yp) != 0:\n",
    "            dice = np.sum(yp[yt==1])*2.0 / (np.sum(yt) + np.sum(yp))\n",
    "            dsc.append(dice)\n",
    "        df=  df.append({'name':name_list[i], 'acc':ac, 'sen':st, 'spe':sp, 'dsc':dice}, ignore_index=True)\n",
    "\n",
    "    print(\"complete\")      \n",
    "    print(\"acc avg : {0:0.4f}\".format(np.mean(acc)))\n",
    "    print(\"sensitivity avg : {0:0.4f}\".format(np.mean(sensitivity)))\n",
    "    print(\"specificity avg : {0:0.4f}\".format(np.mean(specificity)))\n",
    "    print(\"dsc avg : {0:0.4f}\".format(np.mean(dsc)))\n",
    "\n",
    "    predict_save(pred_list,name_list)\n",
    "    return test_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data(test_path, out_rows, out_cols, name, img_type):\n",
    "    print('-'*30)\n",
    "    print('Creating test images...')\n",
    "    print('-'*30)\n",
    "\n",
    "    i = 0\n",
    "    imgs = glob.glob(test_path + \"*.\" + img_type)\n",
    "    imgdatas = np.ndarray((len(imgs),out_rows,out_cols,1), dtype=np.uint8)\n",
    "    imglabels = np.ndarray((len(imgs),out_rows,out_cols,1), dtype=np.uint8)\n",
    "    \n",
    "    imgnames=[]\n",
    "    for j, imgname in enumerate(imgs):\n",
    "        if j%100==0:\n",
    "            print('{}/{}'.format(j, len(imgs)))\n",
    "        midname = imgname[imgname.rindex(\"/\")+1:-4]\n",
    "        img = load_img(imgname, color_mode = \"grayscale\")\n",
    "        label = load_img(imgname.replace('jpg', 'png'), color_mode = \"grayscale\")\n",
    "        img=img.resize((out_rows,out_cols))\n",
    "        label=label.resize((out_rows,out_cols))\n",
    "\n",
    "        img = img_to_array(img)\n",
    "        label = img_to_array(label)\n",
    "        imgdatas[j] = img\n",
    "        imglabels[j] = label\n",
    "        imgnames.append(midname)\n",
    "         \n",
    "    imgdatas = imgdatas.astype('uint8')\n",
    "    imglabels = imglabels.astype('uint8')\n",
    "    \n",
    "    print('img : ', imgdatas.max())\n",
    "    print('mask : ',imglabels.max())\n",
    "    \n",
    "    print('-'*30)\n",
    "    print('normalization start...')\n",
    "    print('-'*30)\n",
    "    imgdatas = imgdatas/255.0\n",
    "    \n",
    "    imglabels[imglabels <= 127] = 0\n",
    "    imglabels[imglabels > 127] = 1\n",
    "    \n",
    "    print('img : ',imgdatas.max())\n",
    "    print('mask : ',imglabels.max())\n",
    "    print('mask : ',imglabels.min())\n",
    "    print('loading done')\n",
    "    return(imgdatas,imglabels,imgnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(test_name):\n",
    "    print(len(test_name))\n",
    "    angle_list = []\n",
    "    dist_list = []\n",
    "    \n",
    "    # pred_img 개인서버 저장경로.\n",
    "    pred_img_path = '/home/hackerton/jupyter/Spine_data/A_pred/'\n",
    "\n",
    "    for i in range(len(test_name)):\n",
    "        img = cv2.imread(pred_img_path+\"%s_pred.png\" %(test_name[i]))\n",
    "        dst = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        contours = cv2.findContours(dst, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        \n",
    "        count = 0\n",
    "        for i in range(len(contours[0])):\n",
    "            if contours[0][i].size < 25:\n",
    "                continue\n",
    "            if count == 2:\n",
    "                break\n",
    "            \n",
    "            rect = cv2.minAreaRect(contours[0][i])\n",
    "            box = cv2.boxPoints(rect)\n",
    "            if (count == 0):\n",
    "                if rect[2] < 45 :\n",
    "                    x1 = box[1][0] # 아래 사각형 왼쪽 위\n",
    "                    y1 = box[1][1]\n",
    "                    x2 = box[2][0] # 아래 사각형 오른쪽 위\n",
    "                    y2 = box[2][1]\n",
    "                elif rect[2] >= 45 : \n",
    "                    x1 = box[0][0] # 아래 사각형 왼쪽 위\n",
    "                    y1 = box[0][1]\n",
    "                    x2 = box[1][0] # 아래 사각형 오른쪽 위\n",
    "                    y2 = box[1][1]\n",
    "                count += 1\n",
    "            elif (count == 1):\n",
    "                if rect[2] < 45 :\n",
    "                    x3 = box[0][0] # 위 사각형 왼쪽 아래\n",
    "                    y3 = box[0][1]\n",
    "                    x4 = box[3][0] # 위 사각형 오른쪽 아래\n",
    "                    y4 = box[3][1]\n",
    "                elif rect[2] >= 45 :\n",
    "                    x3 = box[3][0] # 위 사각형 왼쪽 아래\n",
    "                    y3 = box[3][1]\n",
    "                    x4 = box[2][0] # 위 사각형 오른쪽 아래\n",
    "                    y4 = box[2][1] \n",
    "                count += 1\n",
    "        \n",
    "        l1 = (y1-y2)/(x1-x2)\n",
    "        l2 = (y3-y4)/(x3-x4)\n",
    "        if l1*l2 <= 0:\n",
    "            pred_angle = np.arctan(np.abs(l1))*180/np.pi + np.arctan(np.abs(l2))*180/np.pi\n",
    "        elif l1*l2 > 0:\n",
    "            pred_angle = np.abs(np.arctan(np.abs(l1))*180/np.pi - np.arctan(np.abs(l2))*180/np.pi)                \n",
    "        \n",
    "        angle_list.append(pred_angle)       \n",
    "        \n",
    "        center_x1 = (x1+x2)/2\n",
    "        center_y1 = (y1+y2)/2\n",
    "        center_x2 = (x3+x4)/2\n",
    "        center_y2 = (y3+y4)/2\n",
    "        distance = np.sqrt((center_x1-center_x2)**2 + (center_y1-center_y2)**2)\n",
    "        \n",
    "        dist_list.append(distance) \n",
    "        \n",
    "    return angle_list,dist_list\n",
    "\n",
    "def get_score(angle_list,dist_list,test_name,test_path):\n",
    "    angle_test = []\n",
    "    angle_ai = []\n",
    "    dist_test = []\n",
    "    dist_ai = []\n",
    "    get_no45 = []\n",
    "    \n",
    "    for i in range(len(test_name)):\n",
    "        name = test_name[i]\n",
    "        data = []\n",
    "        \n",
    "        jsonfile = test_path+'/{}.json'.format(name)\n",
    "        for line in open(jsonfile,'r'):\n",
    "            data.append(json.loads(line))\n",
    "        for json_data in data:\n",
    "            check = 0\n",
    "            if json_data['annotation']['ANNOTATION_DATA'] is not None:\n",
    "                for m in json_data['annotation']['ANNOTATION_DATA']:\n",
    "                    if m['type']=='cobbAngle':\n",
    "                        if m['label'] == 'L4-5A':\n",
    "                            angle_test.append(m['angle'])\n",
    "                            angle_ai.append(angle_list[i])\n",
    "                            print(angle_list[i])\n",
    "                            check = 1\n",
    "                    elif m['type']=='line':\n",
    "                        if m['label'] == 'L4-5H':\n",
    "                            dist_test.append(m['distMm'])\n",
    "                            dist_ai.append(dist_list[i])\n",
    "                            check = 1\n",
    "                if check==0:\n",
    "                    get_no45.append(name)\n",
    "    print(get_no45)\n",
    "    print(r2_score(angle_ai, angle_test))\n",
    "    print(r2_score(dist_ai, dist_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Creating training images...\n",
      "------------------------------\n",
      "0/2390\n",
      "1000/2390\n",
      "2000/2390\n",
      "img :  255.0\n",
      "mask :  255.0\n",
      "------------------------------\n",
      "normalization start...\n",
      "------------------------------\n",
      "img :  1.0\n",
      "mask :  1.0\n",
      "mask :  0.0\n",
      "loading done\n",
      "[LogicalDevice(name='/job:localhost/replica:0/task:0/device:GPU:0', device_type='GPU'), LogicalDevice(name='/job:localhost/replica:0/task:0/device:GPU:1', device_type='GPU')]\n",
      "Tensor(\"conv2d_2/Identity:0\", shape=(None, 512, 512, 1), dtype=float32)\n",
      "Tensor(\"conv2d_2_target:0\", shape=(None, None, None, None), dtype=float32)\n",
      "Tensor(\"conv2d_2/Identity:0\", shape=(None, 512, 512, 1), dtype=float32)\n",
      "Tensor(\"conv2d_2_target:0\", shape=(None, None, None, None), dtype=float32)\n",
      "Fitting model...\n",
      "Train on 1912 samples, validate on 478 samples\n",
      "\n",
      "Epoch 00001: CosineAnnealingScheduler setting learng rate to 0.0.\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 230 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "Tensor(\"model_2/conv2d_2/Sigmoid:0\", shape=(4, 512, 512, 1), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(4, 512, 512, 1), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"replica_1/model_2/conv2d_2/Sigmoid:0\", shape=(4, 512, 512, 1), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "Tensor(\"IteratorGetNext_1:1\", shape=(4, 512, 512, 1), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "INFO:tensorflow:batch_all_reduce: 230 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "Tensor(\"model_2/conv2d_2/Sigmoid:0\", shape=(4, 512, 512, 1), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(4, 512, 512, 1), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"replica_1/model_2/conv2d_2/Sigmoid:0\", shape=(4, 512, 512, 1), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "Tensor(\"IteratorGetNext_1:1\", shape=(4, 512, 512, 1), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.8076 - accuracy: 0.5029 - sens: 0.4910 - dice_coef_loss: 0.8076Tensor(\"model_2/conv2d_2/Sigmoid:0\", shape=(None, 512, 512, 1), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"cond_2/Identity_1:0\", shape=(None, 512, 512, 1), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
      "Tensor(\"replica_1/model_2/conv2d_2/Sigmoid:0\", shape=(None, 512, 512, 1), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "Tensor(\"cond_3/Identity_1:0\", shape=(None, 512, 512, 1), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
      "\n",
      "Epoch 00001: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_1_0.807631.hdf5\n",
      "1912/1912 [==============================] - 58s 30ms/sample - loss: 0.8076 - accuracy: 0.5029 - sens: 0.4910 - dice_coef_loss: 0.8076 - val_loss: 0.8086 - val_accuracy: 0.5157 - val_sens: 0.4677 - val_dice_coef_loss: 0.8085\n",
      "\n",
      "Epoch 00002: CosineAnnealingScheduler setting learng rate to 0.0001.\n",
      "Epoch 2/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.7727 - accuracy: 0.5126 - sens: 0.7057 - dice_coef_loss: 0.7727\n",
      "Epoch 00002: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_2_0.772721.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.7727 - accuracy: 0.5128 - sens: 0.7062 - dice_coef_loss: 0.7727 - val_loss: 0.7773 - val_accuracy: 0.7172 - val_sens: 0.4321 - val_dice_coef_loss: 0.7772\n",
      "\n",
      "Epoch 00003: CosineAnnealingScheduler setting learng rate to 0.0002.\n",
      "Epoch 3/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.6520 - accuracy: 0.7192 - sens: 0.8829 - dice_coef_loss: 0.6520\n",
      "Epoch 00003: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_3_0.651794.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.6518 - accuracy: 0.7195 - sens: 0.8828 - dice_coef_loss: 0.6518 - val_loss: 0.7129 - val_accuracy: 0.8319 - val_sens: 0.4168 - val_dice_coef_loss: 0.7130\n",
      "\n",
      "Epoch 00004: CosineAnnealingScheduler setting learng rate to 0.00030000000000000003.\n",
      "Epoch 4/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.5310 - accuracy: 0.8500 - sens: 0.8761 - dice_coef_loss: 0.5310\n",
      "Epoch 00004: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_4_0.530592.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.5306 - accuracy: 0.8500 - sens: 0.8759 - dice_coef_loss: 0.5306 - val_loss: 0.6514 - val_accuracy: 0.8958 - val_sens: 0.3281 - val_dice_coef_loss: 0.6522\n",
      "\n",
      "Epoch 00005: CosineAnnealingScheduler setting learng rate to 0.0004.\n",
      "Epoch 5/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.4098 - accuracy: 0.9019 - sens: 0.8582 - dice_coef_loss: 0.4098\n",
      "Epoch 00005: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_5_0.409616.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.4096 - accuracy: 0.9020 - sens: 0.8584 - dice_coef_loss: 0.4096 - val_loss: 0.8259 - val_accuracy: 0.8888 - val_sens: 0.1073 - val_dice_coef_loss: 0.8264\n",
      "\n",
      "Epoch 00006: CosineAnnealingScheduler setting learng rate to 0.0005.\n",
      "Epoch 6/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.9336 - sens: 0.8356 - dice_coef_loss: 0.3038\n",
      "Epoch 00006: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_6_0.303600.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.3036 - accuracy: 0.9337 - sens: 0.8358 - dice_coef_loss: 0.3036 - val_loss: 0.7529 - val_accuracy: 0.8868 - val_sens: 0.1918 - val_dice_coef_loss: 0.7537\n",
      "\n",
      "Epoch 00007: CosineAnnealingScheduler setting learng rate to 0.0006000000000000001.\n",
      "Epoch 7/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.2452 - accuracy: 0.9449 - sens: 0.8264 - dice_coef_loss: 0.2452\n",
      "Epoch 00007: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_7_0.244954.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.2450 - accuracy: 0.9450 - sens: 0.8266 - dice_coef_loss: 0.2450 - val_loss: 0.4039 - val_accuracy: 0.8967 - val_sens: 0.7118 - val_dice_coef_loss: 0.4038\n",
      "\n",
      "Epoch 00008: CosineAnnealingScheduler setting learng rate to 0.0007.\n",
      "Epoch 8/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.2131 - accuracy: 0.9510 - sens: 0.8307 - dice_coef_loss: 0.2131\n",
      "Epoch 00008: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_8_0.213115.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.2131 - accuracy: 0.9511 - sens: 0.8308 - dice_coef_loss: 0.2131 - val_loss: 0.7673 - val_accuracy: 0.8882 - val_sens: 0.1639 - val_dice_coef_loss: 0.7682\n",
      "\n",
      "Epoch 00009: CosineAnnealingScheduler setting learng rate to 0.0008.\n",
      "Epoch 9/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.1973 - accuracy: 0.9542 - sens: 0.8325 - dice_coef_loss: 0.1973\n",
      "Epoch 00009: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_9_0.197253.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.1973 - accuracy: 0.9542 - sens: 0.8324 - dice_coef_loss: 0.1973 - val_loss: 0.4848 - val_accuracy: 0.9114 - val_sens: 0.4536 - val_dice_coef_loss: 0.4867\n",
      "\n",
      "Epoch 00010: CosineAnnealingScheduler setting learng rate to 0.0009000000000000001.\n",
      "Epoch 10/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.1816 - accuracy: 0.9571 - sens: 0.8418 - dice_coef_loss: 0.1816\n",
      "Epoch 00010: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_10_0.181862.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.1819 - accuracy: 0.9571 - sens: 0.8419 - dice_coef_loss: 0.1819 - val_loss: 0.6812 - val_accuracy: 0.9009 - val_sens: 0.2431 - val_dice_coef_loss: 0.6825\n",
      "\n",
      "Epoch 00011: CosineAnnealingScheduler setting learng rate to 0.0009993147673772868.\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.1702 - accuracy: 0.9596 - sens: 0.8485 - dice_coef_loss: 0.1702\n",
      "Epoch 00011: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_11_0.170050.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.1700 - accuracy: 0.9597 - sens: 0.8488 - dice_coef_loss: 0.1700 - val_loss: 0.3924 - val_accuracy: 0.9286 - val_sens: 0.5347 - val_dice_coef_loss: 0.3949\n",
      "\n",
      "Epoch 00012: CosineAnnealingScheduler setting learng rate to 0.0009972609476841367.\n",
      "Epoch 12/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.1617 - accuracy: 0.9616 - sens: 0.8539 - dice_coef_loss: 0.1617\n",
      "Epoch 00012: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_12_0.161863.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.1619 - accuracy: 0.9616 - sens: 0.8538 - dice_coef_loss: 0.1619 - val_loss: 0.4410 - val_accuracy: 0.9256 - val_sens: 0.4700 - val_dice_coef_loss: 0.4416\n",
      "\n",
      "Epoch 00013: CosineAnnealingScheduler setting learng rate to 0.0009938441702975688.\n",
      "Epoch 13/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.1514 - accuracy: 0.9639 - sens: 0.8626 - dice_coef_loss: 0.1514\n",
      "Epoch 00013: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_13_0.151951.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.1520 - accuracy: 0.9639 - sens: 0.8624 - dice_coef_loss: 0.1520 - val_loss: 0.2973 - val_accuracy: 0.9417 - val_sens: 0.6226 - val_dice_coef_loss: 0.2973\n",
      "\n",
      "Epoch 00014: CosineAnnealingScheduler setting learng rate to 0.0009890738003669028.\n",
      "Epoch 14/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.1464 - accuracy: 0.9651 - sens: 0.8660 - dice_coef_loss: 0.1464\n",
      "Epoch 00014: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_14_0.146692.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.1467 - accuracy: 0.9651 - sens: 0.8656 - dice_coef_loss: 0.1467 - val_loss: 0.2321 - val_accuracy: 0.9510 - val_sens: 0.7444 - val_dice_coef_loss: 0.2322\n",
      "\n",
      "Epoch 00015: CosineAnnealingScheduler setting learng rate to 0.0009829629131445341.\n",
      "Epoch 15/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.1413 - accuracy: 0.9664 - sens: 0.8697 - dice_coef_loss: 0.1413\n",
      "Epoch 00015: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_15_0.141218.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.1412 - accuracy: 0.9665 - sens: 0.8696 - dice_coef_loss: 0.1412 - val_loss: 0.9940 - val_accuracy: 0.8799 - val_sens: 0.0016 - val_dice_coef_loss: 0.9940\n",
      "\n",
      "Epoch 00016: CosineAnnealingScheduler setting learng rate to 0.0009755282581475768.\n",
      "Epoch 16/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.1392 - accuracy: 0.9670 - sens: 0.8694 - dice_coef_loss: 0.1392\n",
      "Epoch 00016: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_16_0.139107.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.1391 - accuracy: 0.9671 - sens: 0.8695 - dice_coef_loss: 0.1391 - val_loss: 0.6691 - val_accuracy: 0.9053 - val_sens: 0.2624 - val_dice_coef_loss: 0.6705\n",
      "\n",
      "Epoch 00017: CosineAnnealingScheduler setting learng rate to 0.0009667902132486009.\n",
      "Epoch 17/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.1318 - accuracy: 0.9687 - sens: 0.8745 - dice_coef_loss: 0.1318\n",
      "Epoch 00017: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_17_0.131941.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.1319 - accuracy: 0.9687 - sens: 0.8741 - dice_coef_loss: 0.1319 - val_loss: 0.2213 - val_accuracy: 0.9546 - val_sens: 0.7258 - val_dice_coef_loss: 0.2212\n",
      "\n",
      "Epoch 00018: CosineAnnealingScheduler setting learng rate to 0.0009567727288213005.\n",
      "Epoch 18/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9705 - sens: 0.8814 - dice_coef_loss: 0.1244\n",
      "Epoch 00018: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_18_0.124192.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.1242 - accuracy: 0.9705 - sens: 0.8817 - dice_coef_loss: 0.1242 - val_loss: 0.2185 - val_accuracy: 0.9549 - val_sens: 0.7788 - val_dice_coef_loss: 0.2184\n",
      "\n",
      "Epoch 00019: CosineAnnealingScheduler setting learng rate to 0.0009455032620941839.\n",
      "Epoch 19/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9715 - sens: 0.8869 - dice_coef_loss: 0.1197\n",
      "Epoch 00019: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_19_0.119669.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.1197 - accuracy: 0.9715 - sens: 0.8871 - dice_coef_loss: 0.1197 - val_loss: 0.6863 - val_accuracy: 0.9002 - val_sens: 0.2296 - val_dice_coef_loss: 0.6876\n",
      "\n",
      "Epoch 00020: CosineAnnealingScheduler setting learng rate to 0.0009330127018922195.\n",
      "Epoch 20/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.1165 - accuracy: 0.9722 - sens: 0.8900 - dice_coef_loss: 0.1165\n",
      "Epoch 00020: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_20_0.116372.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.1164 - accuracy: 0.9723 - sens: 0.8901 - dice_coef_loss: 0.1164 - val_loss: 0.2863 - val_accuracy: 0.9465 - val_sens: 0.6626 - val_dice_coef_loss: 0.2860\n",
      "\n",
      "Epoch 00021: CosineAnnealingScheduler setting learng rate to 0.0009193352839727121.\n",
      "Epoch 21/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.1148 - accuracy: 0.9728 - sens: 0.8903 - dice_coef_loss: 0.1148\n",
      "Epoch 00021: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_21_0.114639.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.1146 - accuracy: 0.9728 - sens: 0.8903 - dice_coef_loss: 0.1146 - val_loss: 0.8304 - val_accuracy: 0.8874 - val_sens: 0.1112 - val_dice_coef_loss: 0.8311\n",
      "\n",
      "Epoch 00022: CosineAnnealingScheduler setting learng rate to 0.0009045084971874737.\n",
      "Epoch 22/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.1117 - accuracy: 0.9735 - sens: 0.8915 - dice_coef_loss: 0.1117\n",
      "Epoch 00022: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_22_0.111599.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.1116 - accuracy: 0.9735 - sens: 0.8916 - dice_coef_loss: 0.1116 - val_loss: 0.2512 - val_accuracy: 0.9513 - val_sens: 0.6747 - val_dice_coef_loss: 0.2535\n",
      "\n",
      "Epoch 00023: CosineAnnealingScheduler setting learng rate to 0.0008885729807284854.\n",
      "Epoch 23/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.1083 - accuracy: 0.9742 - sens: 0.8959 - dice_coef_loss: 0.1083\n",
      "Epoch 00023: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_23_0.108343.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.1083 - accuracy: 0.9742 - sens: 0.8959 - dice_coef_loss: 0.1083 - val_loss: 0.4022 - val_accuracy: 0.9325 - val_sens: 0.4993 - val_dice_coef_loss: 0.4032\n",
      "\n",
      "Epoch 00024: CosineAnnealingScheduler setting learng rate to 0.0008715724127386971.\n",
      "Epoch 24/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.1055 - accuracy: 0.9748 - sens: 0.8976 - dice_coef_loss: 0.1055\n",
      "Epoch 00024: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_24_0.105674.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.1057 - accuracy: 0.9748 - sens: 0.8972 - dice_coef_loss: 0.1057 - val_loss: 0.9408 - val_accuracy: 0.8838 - val_sens: 0.0386 - val_dice_coef_loss: 0.9410\n",
      "\n",
      "Epoch 00025: CosineAnnealingScheduler setting learng rate to 0.0008535533905932737.\n",
      "Epoch 25/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.1061 - accuracy: 0.9747 - sens: 0.8960 - dice_coef_loss: 0.1061\n",
      "Epoch 00025: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_25_0.106019.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.1060 - accuracy: 0.9747 - sens: 0.8960 - dice_coef_loss: 0.1060 - val_loss: 0.6969 - val_accuracy: 0.9002 - val_sens: 0.2150 - val_dice_coef_loss: 0.6971\n",
      "\n",
      "Epoch 00026: CosineAnnealingScheduler setting learng rate to 0.0008345653031794292.\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0993 - accuracy: 0.9765 - sens: 0.9034 - dice_coef_loss: 0.0993\n",
      "Epoch 00026: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_26_0.099425.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0994 - accuracy: 0.9764 - sens: 0.9031 - dice_coef_loss: 0.0994 - val_loss: 0.9972 - val_accuracy: 0.8799 - val_sens: 0.0014 - val_dice_coef_loss: 0.9972\n",
      "\n",
      "Epoch 00027: CosineAnnealingScheduler setting learng rate to 0.0008146601955249188.\n",
      "Epoch 27/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0990 - accuracy: 0.9765 - sens: 0.9043 - dice_coef_loss: 0.0990\n",
      "Epoch 00027: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_27_0.098995.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0990 - accuracy: 0.9765 - sens: 0.9042 - dice_coef_loss: 0.0990 - val_loss: 0.5992 - val_accuracy: 0.9051 - val_sens: 0.2996 - val_dice_coef_loss: 0.6007\n",
      "\n",
      "Epoch 00028: CosineAnnealingScheduler setting learng rate to 0.0007938926261462366.\n",
      "Epoch 28/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0976 - accuracy: 0.9767 - sens: 0.9040 - dice_coef_loss: 0.0976\n",
      "Epoch 00028: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_28_0.097567.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0976 - accuracy: 0.9767 - sens: 0.9042 - dice_coef_loss: 0.0976 - val_loss: 0.9390 - val_accuracy: 0.8815 - val_sens: 0.0363 - val_dice_coef_loss: 0.9391\n",
      "\n",
      "Epoch 00029: CosineAnnealingScheduler setting learng rate to 0.0007723195175075137.\n",
      "Epoch 29/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0930 - accuracy: 0.9780 - sens: 0.9096 - dice_coef_loss: 0.0930\n",
      "Epoch 00029: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_29_0.093133.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0931 - accuracy: 0.9779 - sens: 0.9094 - dice_coef_loss: 0.0931 - val_loss: 0.7577 - val_accuracy: 0.8951 - val_sens: 0.1942 - val_dice_coef_loss: 0.7573\n",
      "\n",
      "Epoch 00030: CosineAnnealingScheduler setting learng rate to 0.00075.\n",
      "Epoch 30/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0940 - accuracy: 0.9777 - sens: 0.9082 - dice_coef_loss: 0.0940\n",
      "Epoch 00030: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_30_0.093941.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0939 - accuracy: 0.9777 - sens: 0.9083 - dice_coef_loss: 0.0939 - val_loss: 0.2354 - val_accuracy: 0.9542 - val_sens: 0.7122 - val_dice_coef_loss: 0.2353\n",
      "\n",
      "Epoch 00031: CosineAnnealingScheduler setting learng rate to 0.0007269952498697733.\n",
      "Epoch 31/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0933 - accuracy: 0.9777 - sens: 0.9084 - dice_coef_loss: 0.0933\n",
      "Epoch 00031: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_31_0.093179.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0932 - accuracy: 0.9777 - sens: 0.9086 - dice_coef_loss: 0.0932 - val_loss: 0.9331 - val_accuracy: 0.8837 - val_sens: 0.0392 - val_dice_coef_loss: 0.9333\n",
      "\n",
      "Epoch 00032: CosineAnnealingScheduler setting learng rate to 0.0007033683215379002.\n",
      "Epoch 32/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0879 - accuracy: 0.9791 - sens: 0.9142 - dice_coef_loss: 0.0879\n",
      "Epoch 00032: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_32_0.088305.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0883 - accuracy: 0.9790 - sens: 0.9138 - dice_coef_loss: 0.0883 - val_loss: 0.1585 - val_accuracy: 0.9632 - val_sens: 0.8298 - val_dice_coef_loss: 0.1586\n",
      "\n",
      "Epoch 00033: CosineAnnealingScheduler setting learng rate to 0.0006791839747726503.\n",
      "Epoch 33/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0895 - accuracy: 0.9788 - sens: 0.9127 - dice_coef_loss: 0.0895\n",
      "Epoch 00033: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_33_0.089336.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0893 - accuracy: 0.9788 - sens: 0.9128 - dice_coef_loss: 0.0893 - val_loss: 0.6668 - val_accuracy: 0.9051 - val_sens: 0.2345 - val_dice_coef_loss: 0.6666\n",
      "\n",
      "Epoch 00034: CosineAnnealingScheduler setting learng rate to 0.0006545084971874737.\n",
      "Epoch 34/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0859 - accuracy: 0.9794 - sens: 0.9172 - dice_coef_loss: 0.0859\n",
      "Epoch 00034: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_34_0.085800.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0858 - accuracy: 0.9795 - sens: 0.9172 - dice_coef_loss: 0.0858 - val_loss: 0.2335 - val_accuracy: 0.9533 - val_sens: 0.6843 - val_dice_coef_loss: 0.2342\n",
      "\n",
      "Epoch 00035: CosineAnnealingScheduler setting learng rate to 0.0006294095225512603.\n",
      "Epoch 35/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0858 - accuracy: 0.9795 - sens: 0.9170 - dice_coef_loss: 0.0858\n",
      "Epoch 00035: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_35_0.085756.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0858 - accuracy: 0.9795 - sens: 0.9171 - dice_coef_loss: 0.0858 - val_loss: 0.1414 - val_accuracy: 0.9670 - val_sens: 0.8574 - val_dice_coef_loss: 0.1415\n",
      "\n",
      "Epoch 00036: CosineAnnealingScheduler setting learng rate to 0.0006039558454088796.\n",
      "Epoch 36/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0836 - accuracy: 0.9801 - sens: 0.9182 - dice_coef_loss: 0.0836\n",
      "Epoch 00036: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_36_0.083589.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0836 - accuracy: 0.9801 - sens: 0.9181 - dice_coef_loss: 0.0836 - val_loss: 0.3114 - val_accuracy: 0.9433 - val_sens: 0.5895 - val_dice_coef_loss: 0.3110\n",
      "\n",
      "Epoch 00037: CosineAnnealingScheduler setting learng rate to 0.0005782172325201155.\n",
      "Epoch 37/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9808 - sens: 0.9208 - dice_coef_loss: 0.0807\n",
      "Epoch 00037: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_37_0.080647.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0806 - accuracy: 0.9808 - sens: 0.9208 - dice_coef_loss: 0.0806 - val_loss: 0.2274 - val_accuracy: 0.9536 - val_sens: 0.7156 - val_dice_coef_loss: 0.2280\n",
      "\n",
      "Epoch 00038: CosineAnnealingScheduler setting learng rate to 0.0005522642316338268.\n",
      "Epoch 38/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0808 - accuracy: 0.9808 - sens: 0.9211 - dice_coef_loss: 0.0808\n",
      "Epoch 00038: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_38_0.080766.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0808 - accuracy: 0.9809 - sens: 0.9212 - dice_coef_loss: 0.0808 - val_loss: 0.1233 - val_accuracy: 0.9705 - val_sens: 0.8832 - val_dice_coef_loss: 0.1233\n",
      "\n",
      "Epoch 00039: CosineAnnealingScheduler setting learng rate to 0.0005261679781214719.\n",
      "Epoch 39/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0791 - accuracy: 0.9812 - sens: 0.9236 - dice_coef_loss: 0.0791\n",
      "Epoch 00039: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_39_0.079076.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0791 - accuracy: 0.9812 - sens: 0.9235 - dice_coef_loss: 0.0791 - val_loss: 0.1957 - val_accuracy: 0.9577 - val_sens: 0.7521 - val_dice_coef_loss: 0.1954\n",
      "\n",
      "Epoch 00040: CosineAnnealingScheduler setting learng rate to 0.0005000000000000001.\n",
      "Epoch 40/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9814 - sens: 0.9245 - dice_coef_loss: 0.0779\n",
      "Epoch 00040: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_40_0.077916.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0779 - accuracy: 0.9815 - sens: 0.9245 - dice_coef_loss: 0.0779 - val_loss: 0.3835 - val_accuracy: 0.9366 - val_sens: 0.5377 - val_dice_coef_loss: 0.3829\n",
      "\n",
      "Epoch 00041: CosineAnnealingScheduler setting learng rate to 0.0004738320218785282.\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0771 - accuracy: 0.9816 - sens: 0.9255 - dice_coef_loss: 0.0771\n",
      "Epoch 00041: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_41_0.077187.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0772 - accuracy: 0.9815 - sens: 0.9255 - dice_coef_loss: 0.0772 - val_loss: 0.1792 - val_accuracy: 0.9601 - val_sens: 0.7953 - val_dice_coef_loss: 0.1796\n",
      "\n",
      "Epoch 00042: CosineAnnealingScheduler setting learng rate to 0.00044773576836617336.\n",
      "Epoch 42/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9822 - sens: 0.9284 - dice_coef_loss: 0.0745\n",
      "Epoch 00042: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_42_0.074499.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0745 - accuracy: 0.9822 - sens: 0.9283 - dice_coef_loss: 0.0745 - val_loss: 0.3524 - val_accuracy: 0.9397 - val_sens: 0.5448 - val_dice_coef_loss: 0.3533\n",
      "\n",
      "Epoch 00043: CosineAnnealingScheduler setting learng rate to 0.0004217827674798846.\n",
      "Epoch 43/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0740 - accuracy: 0.9824 - sens: 0.9278 - dice_coef_loss: 0.0740\n",
      "Epoch 00043: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_43_0.074015.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0740 - accuracy: 0.9824 - sens: 0.9277 - dice_coef_loss: 0.0740 - val_loss: 0.1376 - val_accuracy: 0.9684 - val_sens: 0.8587 - val_dice_coef_loss: 0.1374\n",
      "\n",
      "Epoch 00044: CosineAnnealingScheduler setting learng rate to 0.0003960441545911203.\n",
      "Epoch 44/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9826 - sens: 0.9293 - dice_coef_loss: 0.0729\n",
      "Epoch 00044: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_44_0.072814.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0728 - accuracy: 0.9826 - sens: 0.9293 - dice_coef_loss: 0.0728 - val_loss: 0.2930 - val_accuracy: 0.9401 - val_sens: 0.6379 - val_dice_coef_loss: 0.2929\n",
      "\n",
      "Epoch 00045: CosineAnnealingScheduler setting learng rate to 0.0003705904774487397.\n",
      "Epoch 45/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9826 - sens: 0.9284 - dice_coef_loss: 0.0730\n",
      "Epoch 00045: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_45_0.072901.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0729 - accuracy: 0.9826 - sens: 0.9285 - dice_coef_loss: 0.0729 - val_loss: 0.1611 - val_accuracy: 0.9643 - val_sens: 0.8233 - val_dice_coef_loss: 0.1609\n",
      "\n",
      "Epoch 00046: CosineAnnealingScheduler setting learng rate to 0.00034549150281252633.\n",
      "Epoch 46/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0717 - accuracy: 0.9829 - sens: 0.9298 - dice_coef_loss: 0.0717\n",
      "Epoch 00046: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_46_0.071676.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0717 - accuracy: 0.9829 - sens: 0.9299 - dice_coef_loss: 0.0717 - val_loss: 0.1397 - val_accuracy: 0.9674 - val_sens: 0.8572 - val_dice_coef_loss: 0.1395\n",
      "\n",
      "Epoch 00047: CosineAnnealingScheduler setting learng rate to 0.00032081602522734986.\n",
      "Epoch 47/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0703 - accuracy: 0.9832 - sens: 0.9311 - dice_coef_loss: 0.0703\n",
      "Epoch 00047: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_47_0.070276.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0703 - accuracy: 0.9832 - sens: 0.9312 - dice_coef_loss: 0.0703 - val_loss: 0.1333 - val_accuracy: 0.9686 - val_sens: 0.8855 - val_dice_coef_loss: 0.1332\n",
      "\n",
      "Epoch 00048: CosineAnnealingScheduler setting learng rate to 0.0002966316784621.\n",
      "Epoch 48/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 0.9833 - sens: 0.9321 - dice_coef_loss: 0.0697\n",
      "Epoch 00048: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_48_0.069802.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0698 - accuracy: 0.9833 - sens: 0.9322 - dice_coef_loss: 0.0698 - val_loss: 0.1230 - val_accuracy: 0.9710 - val_sens: 0.8747 - val_dice_coef_loss: 0.1231\n",
      "\n",
      "Epoch 00049: CosineAnnealingScheduler setting learng rate to 0.00027300475013022663.\n",
      "Epoch 49/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0692 - accuracy: 0.9835 - sens: 0.9325 - dice_coef_loss: 0.0692\n",
      "Epoch 00049: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_49_0.069282.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0693 - accuracy: 0.9835 - sens: 0.9324 - dice_coef_loss: 0.0693 - val_loss: 0.2960 - val_accuracy: 0.9466 - val_sens: 0.6255 - val_dice_coef_loss: 0.2964\n",
      "\n",
      "Epoch 00050: CosineAnnealingScheduler setting learng rate to 0.0002500000000000001.\n",
      "Epoch 50/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0681 - accuracy: 0.9838 - sens: 0.9339 - dice_coef_loss: 0.0681\n",
      "Epoch 00050: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_50_0.068112.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0681 - accuracy: 0.9838 - sens: 0.9339 - dice_coef_loss: 0.0681 - val_loss: 0.1241 - val_accuracy: 0.9705 - val_sens: 0.8868 - val_dice_coef_loss: 0.1241\n",
      "\n",
      "Epoch 00051: CosineAnnealingScheduler setting learng rate to 0.00022768048249248663.\n",
      "Epoch 51/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9839 - sens: 0.9349 - dice_coef_loss: 0.0676\n",
      "Epoch 00051: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_51_0.067556.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0676 - accuracy: 0.9839 - sens: 0.9350 - dice_coef_loss: 0.0676 - val_loss: 0.1361 - val_accuracy: 0.9691 - val_sens: 0.8424 - val_dice_coef_loss: 0.1362\n",
      "\n",
      "Epoch 00052: CosineAnnealingScheduler setting learng rate to 0.00020610737385376348.\n",
      "Epoch 52/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0671 - accuracy: 0.9840 - sens: 0.9350 - dice_coef_loss: 0.0671\n",
      "Epoch 00052: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_52_0.067117.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0671 - accuracy: 0.9840 - sens: 0.9350 - dice_coef_loss: 0.0671 - val_loss: 0.1447 - val_accuracy: 0.9677 - val_sens: 0.8479 - val_dice_coef_loss: 0.1446\n",
      "\n",
      "Epoch 00053: CosineAnnealingScheduler setting learng rate to 0.00018533980447508135.\n",
      "Epoch 53/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0663 - accuracy: 0.9842 - sens: 0.9355 - dice_coef_loss: 0.0663\n",
      "Epoch 00053: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_53_0.066242.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0662 - accuracy: 0.9842 - sens: 0.9356 - dice_coef_loss: 0.0662 - val_loss: 0.1220 - val_accuracy: 0.9713 - val_sens: 0.8855 - val_dice_coef_loss: 0.1219\n",
      "\n",
      "Epoch 00054: CosineAnnealingScheduler setting learng rate to 0.00016543469682057105.\n",
      "Epoch 54/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0660 - accuracy: 0.9842 - sens: 0.9358 - dice_coef_loss: 0.0660\n",
      "Epoch 00054: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_54_0.066028.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0660 - accuracy: 0.9842 - sens: 0.9359 - dice_coef_loss: 0.0660 - val_loss: 0.1283 - val_accuracy: 0.9694 - val_sens: 0.8792 - val_dice_coef_loss: 0.1281\n",
      "\n",
      "Epoch 00055: CosineAnnealingScheduler setting learng rate to 0.00014644660940672628.\n",
      "Epoch 55/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0642 - accuracy: 0.9846 - sens: 0.9381 - dice_coef_loss: 0.0642\n",
      "Epoch 00055: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_55_0.064288.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0643 - accuracy: 0.9846 - sens: 0.9380 - dice_coef_loss: 0.0643 - val_loss: 0.1167 - val_accuracy: 0.9721 - val_sens: 0.8864 - val_dice_coef_loss: 0.1167\n",
      "\n",
      "Epoch 00056: CosineAnnealingScheduler setting learng rate to 0.000128427587261303.\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9846 - sens: 0.9371 - dice_coef_loss: 0.0647\n",
      "Epoch 00056: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_56_0.064777.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0648 - accuracy: 0.9845 - sens: 0.9368 - dice_coef_loss: 0.0648 - val_loss: 0.1314 - val_accuracy: 0.9697 - val_sens: 0.8694 - val_dice_coef_loss: 0.1312\n",
      "\n",
      "Epoch 00057: CosineAnnealingScheduler setting learng rate to 0.00011142701927151455.\n",
      "Epoch 57/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0639 - accuracy: 0.9847 - sens: 0.9382 - dice_coef_loss: 0.0639\n",
      "Epoch 00057: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_57_0.063825.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0638 - accuracy: 0.9847 - sens: 0.9382 - dice_coef_loss: 0.0638 - val_loss: 0.1346 - val_accuracy: 0.9694 - val_sens: 0.8485 - val_dice_coef_loss: 0.1346\n",
      "\n",
      "Epoch 00058: CosineAnnealingScheduler setting learng rate to 9.549150281252633e-05.\n",
      "Epoch 58/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0641 - accuracy: 0.9847 - sens: 0.9379 - dice_coef_loss: 0.0641\n",
      "Epoch 00058: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_58_0.064118.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0641 - accuracy: 0.9847 - sens: 0.9379 - dice_coef_loss: 0.0641 - val_loss: 0.1196 - val_accuracy: 0.9720 - val_sens: 0.8700 - val_dice_coef_loss: 0.1196\n",
      "\n",
      "Epoch 00059: CosineAnnealingScheduler setting learng rate to 8.066471602728804e-05.\n",
      "Epoch 59/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0640 - accuracy: 0.9848 - sens: 0.9378 - dice_coef_loss: 0.0640\n",
      "Epoch 00059: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_59_0.064090.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0641 - accuracy: 0.9848 - sens: 0.9376 - dice_coef_loss: 0.0641 - val_loss: 0.1338 - val_accuracy: 0.9692 - val_sens: 0.8456 - val_dice_coef_loss: 0.1337\n",
      "\n",
      "Epoch 00060: CosineAnnealingScheduler setting learng rate to 6.698729810778065e-05.\n",
      "Epoch 60/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0637 - accuracy: 0.9848 - sens: 0.9390 - dice_coef_loss: 0.0637\n",
      "Epoch 00060: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_60_0.063655.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0637 - accuracy: 0.9848 - sens: 0.9390 - dice_coef_loss: 0.0637 - val_loss: 0.1174 - val_accuracy: 0.9721 - val_sens: 0.8849 - val_dice_coef_loss: 0.1174\n",
      "\n",
      "Epoch 00061: CosineAnnealingScheduler setting learng rate to 5.449673790581611e-05.\n",
      "Epoch 61/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0628 - accuracy: 0.9850 - sens: 0.9394 - dice_coef_loss: 0.0628\n",
      "Epoch 00061: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_61_0.062855.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0629 - accuracy: 0.9850 - sens: 0.9393 - dice_coef_loss: 0.0629 - val_loss: 0.1242 - val_accuracy: 0.9709 - val_sens: 0.8817 - val_dice_coef_loss: 0.1242\n",
      "\n",
      "Epoch 00062: CosineAnnealingScheduler setting learng rate to 4.322727117869951e-05.\n",
      "Epoch 62/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0629 - accuracy: 0.9850 - sens: 0.9391 - dice_coef_loss: 0.0629\n",
      "Epoch 00062: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_62_0.062866.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0629 - accuracy: 0.9850 - sens: 0.9391 - dice_coef_loss: 0.0629 - val_loss: 0.1180 - val_accuracy: 0.9722 - val_sens: 0.8799 - val_dice_coef_loss: 0.1180\n",
      "\n",
      "Epoch 00063: CosineAnnealingScheduler setting learng rate to 3.320978675139913e-05.\n",
      "Epoch 63/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0625 - accuracy: 0.9851 - sens: 0.9398 - dice_coef_loss: 0.0625\n",
      "Epoch 00063: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_63_0.062468.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0625 - accuracy: 0.9851 - sens: 0.9398 - dice_coef_loss: 0.0625 - val_loss: 0.1166 - val_accuracy: 0.9722 - val_sens: 0.8886 - val_dice_coef_loss: 0.1166\n",
      "\n",
      "Epoch 00064: CosineAnnealingScheduler setting learng rate to 2.4471741852423235e-05.\n",
      "Epoch 64/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0629 - accuracy: 0.9850 - sens: 0.9395 - dice_coef_loss: 0.0629\n",
      "Epoch 00064: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_64_0.062905.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0629 - accuracy: 0.9850 - sens: 0.9395 - dice_coef_loss: 0.0629 - val_loss: 0.1178 - val_accuracy: 0.9721 - val_sens: 0.8853 - val_dice_coef_loss: 0.1177\n",
      "\n",
      "Epoch 00065: CosineAnnealingScheduler setting learng rate to 1.7037086855465843e-05.\n",
      "Epoch 65/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0622 - accuracy: 0.9851 - sens: 0.9397 - dice_coef_loss: 0.0622\n",
      "Epoch 00065: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_65_0.062240.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0622 - accuracy: 0.9851 - sens: 0.9397 - dice_coef_loss: 0.0622 - val_loss: 0.1169 - val_accuracy: 0.9722 - val_sens: 0.8876 - val_dice_coef_loss: 0.1169\n",
      "\n",
      "Epoch 00066: CosineAnnealingScheduler setting learng rate to 1.0926199633097156e-05.\n",
      "Epoch 66/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9852 - sens: 0.9407 - dice_coef_loss: 0.0621\n",
      "Epoch 00066: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_66_0.062213.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0622 - accuracy: 0.9852 - sens: 0.9405 - dice_coef_loss: 0.0622 - val_loss: 0.1164 - val_accuracy: 0.9724 - val_sens: 0.8861 - val_dice_coef_loss: 0.1164\n",
      "\n",
      "Epoch 00067: CosineAnnealingScheduler setting learng rate to 6.15582970243117e-06.\n",
      "Epoch 67/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9852 - sens: 0.9395 - dice_coef_loss: 0.0623\n",
      "Epoch 00067: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_67_0.062305.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0623 - accuracy: 0.9852 - sens: 0.9395 - dice_coef_loss: 0.0623 - val_loss: 0.1177 - val_accuracy: 0.9721 - val_sens: 0.8873 - val_dice_coef_loss: 0.1177\n",
      "\n",
      "Epoch 00068: CosineAnnealingScheduler setting learng rate to 2.7390523158632995e-06.\n",
      "Epoch 68/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9851 - sens: 0.9404 - dice_coef_loss: 0.0621\n",
      "Epoch 00068: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_68_0.062124.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0621 - accuracy: 0.9851 - sens: 0.9404 - dice_coef_loss: 0.0621 - val_loss: 0.1171 - val_accuracy: 0.9722 - val_sens: 0.8876 - val_dice_coef_loss: 0.1171\n",
      "\n",
      "Epoch 00069: CosineAnnealingScheduler setting learng rate to 6.852326227130834e-07.\n",
      "Epoch 69/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9851 - sens: 0.9398 - dice_coef_loss: 0.0621\n",
      "Epoch 00069: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_69_0.062081.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0621 - accuracy: 0.9852 - sens: 0.9398 - dice_coef_loss: 0.0621 - val_loss: 0.1176 - val_accuracy: 0.9721 - val_sens: 0.8880 - val_dice_coef_loss: 0.1176\n",
      "\n",
      "Epoch 00070: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 70/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0627 - accuracy: 0.9851 - sens: 0.9398 - dice_coef_loss: 0.0627\n",
      "Epoch 00070: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_70_0.062691.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0627 - accuracy: 0.9851 - sens: 0.9399 - dice_coef_loss: 0.0627 - val_loss: 0.1177 - val_accuracy: 0.9721 - val_sens: 0.8872 - val_dice_coef_loss: 0.1176\n",
      "\n",
      "Epoch 00071: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0627 - accuracy: 0.9851 - sens: 0.9397 - dice_coef_loss: 0.0627\n",
      "Epoch 00071: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_71_0.062798.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0628 - accuracy: 0.9850 - sens: 0.9395 - dice_coef_loss: 0.0628 - val_loss: 0.1179 - val_accuracy: 0.9720 - val_sens: 0.8864 - val_dice_coef_loss: 0.1179\n",
      "\n",
      "Epoch 00072: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 72/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9852 - sens: 0.9401 - dice_coef_loss: 0.0620\n",
      "Epoch 00072: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_72_0.062057.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0621 - accuracy: 0.9852 - sens: 0.9402 - dice_coef_loss: 0.0621 - val_loss: 0.1172 - val_accuracy: 0.9722 - val_sens: 0.8872 - val_dice_coef_loss: 0.1172\n",
      "\n",
      "Epoch 00073: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 73/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9852 - sens: 0.9400 - dice_coef_loss: 0.0620\n",
      "Epoch 00073: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_73_0.061991.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0620 - accuracy: 0.9851 - sens: 0.9400 - dice_coef_loss: 0.0620 - val_loss: 0.1173 - val_accuracy: 0.9722 - val_sens: 0.8867 - val_dice_coef_loss: 0.1173\n",
      "\n",
      "Epoch 00074: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 74/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0622 - accuracy: 0.9852 - sens: 0.9398 - dice_coef_loss: 0.0622\n",
      "Epoch 00074: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_74_0.062126.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0621 - accuracy: 0.9852 - sens: 0.9398 - dice_coef_loss: 0.0621 - val_loss: 0.1185 - val_accuracy: 0.9719 - val_sens: 0.8862 - val_dice_coef_loss: 0.1185\n",
      "\n",
      "Epoch 00075: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 75/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0624 - accuracy: 0.9852 - sens: 0.9401 - dice_coef_loss: 0.0624\n",
      "Epoch 00075: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_75_0.062432.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0624 - accuracy: 0.9852 - sens: 0.9401 - dice_coef_loss: 0.0624 - val_loss: 0.1176 - val_accuracy: 0.9721 - val_sens: 0.8869 - val_dice_coef_loss: 0.1176\n",
      "\n",
      "Epoch 00076: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 76/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9852 - sens: 0.9403 - dice_coef_loss: 0.0621\n",
      "Epoch 00076: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_76_0.062072.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0621 - accuracy: 0.9852 - sens: 0.9402 - dice_coef_loss: 0.0621 - val_loss: 0.1175 - val_accuracy: 0.9721 - val_sens: 0.8867 - val_dice_coef_loss: 0.1175\n",
      "\n",
      "Epoch 00077: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 77/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0627 - accuracy: 0.9850 - sens: 0.9398 - dice_coef_loss: 0.0627\n",
      "Epoch 00077: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_77_0.062714.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0627 - accuracy: 0.9851 - sens: 0.9398 - dice_coef_loss: 0.0627 - val_loss: 0.1179 - val_accuracy: 0.9721 - val_sens: 0.8857 - val_dice_coef_loss: 0.1179\n",
      "\n",
      "Epoch 00078: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 78/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9851 - sens: 0.9393 - dice_coef_loss: 0.0626\n",
      "Epoch 00078: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_78_0.062546.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0625 - accuracy: 0.9851 - sens: 0.9393 - dice_coef_loss: 0.0625 - val_loss: 0.1177 - val_accuracy: 0.9721 - val_sens: 0.8869 - val_dice_coef_loss: 0.1177\n",
      "\n",
      "Epoch 00079: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 79/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0619 - accuracy: 0.9852 - sens: 0.9402 - dice_coef_loss: 0.0619\n",
      "Epoch 00079: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_79_0.062021.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0620 - accuracy: 0.9852 - sens: 0.9401 - dice_coef_loss: 0.0620 - val_loss: 0.1177 - val_accuracy: 0.9721 - val_sens: 0.8873 - val_dice_coef_loss: 0.1177\n",
      "\n",
      "Epoch 00080: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 80/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0622 - accuracy: 0.9851 - sens: 0.9398 - dice_coef_loss: 0.0622\n",
      "Epoch 00080: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_80_0.062154.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0622 - accuracy: 0.9851 - sens: 0.9399 - dice_coef_loss: 0.0622 - val_loss: 0.1172 - val_accuracy: 0.9722 - val_sens: 0.8869 - val_dice_coef_loss: 0.1171\n",
      "\n",
      "Epoch 00081: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 81/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0622 - accuracy: 0.9851 - sens: 0.9397 - dice_coef_loss: 0.0622\n",
      "Epoch 00081: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_81_0.062122.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0621 - accuracy: 0.9851 - sens: 0.9398 - dice_coef_loss: 0.0621 - val_loss: 0.1177 - val_accuracy: 0.9721 - val_sens: 0.8863 - val_dice_coef_loss: 0.1177\n",
      "\n",
      "Epoch 00082: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 82/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0619 - accuracy: 0.9852 - sens: 0.9401 - dice_coef_loss: 0.0619\n",
      "Epoch 00082: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_82_0.061920.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0619 - accuracy: 0.9852 - sens: 0.9401 - dice_coef_loss: 0.0619 - val_loss: 0.1173 - val_accuracy: 0.9722 - val_sens: 0.8861 - val_dice_coef_loss: 0.1173\n",
      "\n",
      "Epoch 00083: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 83/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0616 - accuracy: 0.9852 - sens: 0.9406 - dice_coef_loss: 0.0616\n",
      "Epoch 00083: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_83_0.061621.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0616 - accuracy: 0.9852 - sens: 0.9406 - dice_coef_loss: 0.0616 - val_loss: 0.1172 - val_accuracy: 0.9722 - val_sens: 0.8867 - val_dice_coef_loss: 0.1172\n",
      "\n",
      "Epoch 00084: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 84/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9851 - sens: 0.9402 - dice_coef_loss: 0.0621\n",
      "Epoch 00084: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_84_0.062132.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0621 - accuracy: 0.9851 - sens: 0.9402 - dice_coef_loss: 0.0621 - val_loss: 0.1177 - val_accuracy: 0.9720 - val_sens: 0.8871 - val_dice_coef_loss: 0.1177\n",
      "\n",
      "Epoch 00085: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 85/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9851 - sens: 0.9397 - dice_coef_loss: 0.0623\n",
      "Epoch 00085: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_85_0.062310.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0623 - accuracy: 0.9851 - sens: 0.9397 - dice_coef_loss: 0.0623 - val_loss: 0.1182 - val_accuracy: 0.9720 - val_sens: 0.8858 - val_dice_coef_loss: 0.1182\n",
      "\n",
      "Epoch 00086: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 86/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9851 - sens: 0.9396 - dice_coef_loss: 0.0626\n",
      "Epoch 00086: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_86_0.062735.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0627 - accuracy: 0.9850 - sens: 0.9394 - dice_coef_loss: 0.0627 - val_loss: 0.1182 - val_accuracy: 0.9720 - val_sens: 0.8856 - val_dice_coef_loss: 0.1181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00087: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 87/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9851 - sens: 0.9400 - dice_coef_loss: 0.0621\n",
      "Epoch 00087: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_87_0.062165.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0622 - accuracy: 0.9851 - sens: 0.9400 - dice_coef_loss: 0.0622 - val_loss: 0.1174 - val_accuracy: 0.9721 - val_sens: 0.8875 - val_dice_coef_loss: 0.1174\n",
      "\n",
      "Epoch 00088: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 88/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0625 - accuracy: 0.9851 - sens: 0.9394 - dice_coef_loss: 0.0625\n",
      "Epoch 00088: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_88_0.062570.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0626 - accuracy: 0.9851 - sens: 0.9394 - dice_coef_loss: 0.0626 - val_loss: 0.1178 - val_accuracy: 0.9720 - val_sens: 0.8868 - val_dice_coef_loss: 0.1178\n",
      "\n",
      "Epoch 00089: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 89/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0624 - accuracy: 0.9851 - sens: 0.9399 - dice_coef_loss: 0.0624\n",
      "Epoch 00089: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_89_0.062394.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0624 - accuracy: 0.9851 - sens: 0.9399 - dice_coef_loss: 0.0624 - val_loss: 0.1173 - val_accuracy: 0.9722 - val_sens: 0.8861 - val_dice_coef_loss: 0.1173\n",
      "\n",
      "Epoch 00090: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 90/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0631 - accuracy: 0.9850 - sens: 0.9391 - dice_coef_loss: 0.0631\n",
      "Epoch 00090: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_90_0.063059.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0631 - accuracy: 0.9850 - sens: 0.9392 - dice_coef_loss: 0.0631 - val_loss: 0.1180 - val_accuracy: 0.9720 - val_sens: 0.8867 - val_dice_coef_loss: 0.1180\n",
      "\n",
      "Epoch 00091: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 91/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0619 - accuracy: 0.9852 - sens: 0.9400 - dice_coef_loss: 0.0619\n",
      "Epoch 00091: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_91_0.061895.hdf5\n",
      "1912/1912 [==============================] - 17s 9ms/sample - loss: 0.0619 - accuracy: 0.9852 - sens: 0.9400 - dice_coef_loss: 0.0619 - val_loss: 0.1176 - val_accuracy: 0.9721 - val_sens: 0.8863 - val_dice_coef_loss: 0.1176\n",
      "\n",
      "Epoch 00092: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 92/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9851 - sens: 0.9398 - dice_coef_loss: 0.0623\n",
      "Epoch 00092: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_92_0.062187.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0622 - accuracy: 0.9852 - sens: 0.9398 - dice_coef_loss: 0.0622 - val_loss: 0.1173 - val_accuracy: 0.9722 - val_sens: 0.8863 - val_dice_coef_loss: 0.1173\n",
      "\n",
      "Epoch 00093: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 93/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0619 - accuracy: 0.9852 - sens: 0.9398 - dice_coef_loss: 0.0619\n",
      "Epoch 00093: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_93_0.061884.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0619 - accuracy: 0.9852 - sens: 0.9399 - dice_coef_loss: 0.0619 - val_loss: 0.1175 - val_accuracy: 0.9721 - val_sens: 0.8870 - val_dice_coef_loss: 0.1175\n",
      "\n",
      "Epoch 00094: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 94/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0627 - accuracy: 0.9851 - sens: 0.9398 - dice_coef_loss: 0.0627\n",
      "Epoch 00094: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_94_0.062744.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0627 - accuracy: 0.9850 - sens: 0.9397 - dice_coef_loss: 0.0627 - val_loss: 0.1176 - val_accuracy: 0.9721 - val_sens: 0.8856 - val_dice_coef_loss: 0.1176\n",
      "\n",
      "Epoch 00095: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 95/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9851 - sens: 0.9404 - dice_coef_loss: 0.0623\n",
      "Epoch 00095: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_95_0.062385.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0624 - accuracy: 0.9851 - sens: 0.9402 - dice_coef_loss: 0.0624 - val_loss: 0.1184 - val_accuracy: 0.9719 - val_sens: 0.8857 - val_dice_coef_loss: 0.1184\n",
      "\n",
      "Epoch 00096: CosineAnnealingScheduler setting learng rate to 1e-06.\n",
      "Epoch 96/100\n",
      "1904/1912 [============================>.] - ETA: 0s - loss: 0.0622 - accuracy: 0.9852 - sens: 0.9397 - dice_coef_loss: 0.0622\n",
      "Epoch 00096: saving model to /home/hackerton/jupyter/Spine_data/A_check/final3_96_0.062142.hdf5\n",
      "1912/1912 [==============================] - 18s 9ms/sample - loss: 0.0621 - accuracy: 0.9852 - sens: 0.9397 - dice_coef_loss: 0.0621 - val_loss: 0.1181 - val_accuracy: 0.9720 - val_sens: 0.8862 - val_dice_coef_loss: 0.1180\n",
      "save model\n",
      "------------------------------\n",
      "Creating test images...\n",
      "------------------------------\n",
      "0/239\n",
      "100/239\n",
      "200/239\n",
      "img :  255\n",
      "mask :  255\n",
      "------------------------------\n",
      "normalization start...\n",
      "------------------------------\n",
      "img :  1.0\n",
      "mask :  1\n",
      "mask :  0\n",
      "loading done\n",
      "predict test data\n",
      "239/1 [==================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 6s 26ms/sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(239, 512, 512, 1)\n",
      "[0.0000000e+00 2.9802322e-08 5.9604645e-08 ... 9.9999988e-01 9.9999994e-01\n",
      " 1.0000000e+00]\n",
      "complete\n",
      "acc avg : 0.9911\n",
      "sensitivity avg : 0.8603\n",
      "specificity avg : 0.9957\n",
      "dsc avg : 0.8651\n",
      "239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hackerton/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in float_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.121705414325575\n",
      "3.576328243997108\n",
      "1.5785728304058377\n",
      "4.289126572336227\n",
      "3.583330682000252\n",
      "25.20114065982636\n",
      "8.014034055782739\n",
      "20.39282238037349\n",
      "30.411054852568938\n",
      "36.86987340391658\n",
      "22.011295681806665\n",
      "11.104469045527487\n",
      "12.80424230209473\n",
      "11.449354325025404\n",
      "9.462305873288361\n",
      "2.4194839497898277\n",
      "14.869960445805155\n",
      "0.0\n",
      "8.698153913900855\n",
      "6.6048142717434875\n",
      "4.969789074073848\n",
      "23.36068941201895\n",
      "6.241274464961795\n",
      "7.594630772109748\n",
      "8.343909354249316\n",
      "5.8308219307526965\n",
      "6.095866567721394\n",
      "9.491105579367131\n",
      "12.753276711168564\n",
      "16.025589179783584\n",
      "15.914374810353852\n",
      "12.528808093624797\n",
      "11.003549158559139\n",
      "5.303703789104255\n",
      "18.935289466683567\n",
      "7.823106576262198\n",
      "6.911214850362967\n",
      "16.440402789777735\n",
      "12.634528322919193\n",
      "21.801420830641685\n",
      "8.574257484007997\n",
      "12.447254354274374\n",
      "18.86083784340949\n",
      "0.41518286709615726\n",
      "12.777839992414634\n",
      "4.172527559241293\n",
      "18.434964022714972\n",
      "0.6780170840947104\n",
      "15.851801206211546\n",
      "12.094781588382219\n",
      "7.281511635224503\n",
      "6.115517448850344\n",
      "27.70174036308608\n",
      "7.125027092141045\n",
      "4.517113270911081\n",
      "1.3918149679301788\n",
      "1.7356806282510586\n",
      "1.28573252516636\n",
      "15.108293493590402\n",
      "12.528818338908552\n",
      "3.1221315498052498\n",
      "9.527258410974245\n",
      "23.96251317460123\n",
      "0.0\n",
      "8.143756210150794\n",
      "12.528820046455843\n",
      "8.530751740315955\n",
      "7.125035629877508\n",
      "19.467195784383602\n",
      "9.868591138330107\n",
      "10.933827441285295\n",
      "2.9028303973888114e-05\n",
      "8.56665377591448\n",
      "24.531867071642914\n",
      "13.570417064693618\n",
      "6.340169200841234\n",
      "11.040939321623082\n",
      "3.270502681928379\n",
      "17.102785143975467\n",
      "13.073913538452466\n",
      "36.86991353127795\n",
      "38.87749310477323\n",
      "2.862419156144216\n",
      "2.385923174972735\n",
      "6.0741060119125585\n",
      "15.25513871876427\n",
      "7.124992514308372\n",
      "1.622169927806283e-05\n",
      "12.420456960839475\n",
      "1.548166429046527\n",
      "23.887690586677856\n",
      "15.524104738090202\n",
      "28.20331231657821\n",
      "12.466561164623883\n",
      "14.036218890615562\n",
      "3.0940823107615345\n",
      "12.094770916211642\n",
      "2.3859312858223745\n",
      "11.692375443902218\n",
      "9.353001929113823\n",
      "26.5649262389501\n",
      "41.07851857660208\n",
      "1.6522816706076764\n",
      "38.659915721215185\n",
      "12.5564325802795\n",
      "30.9637746776607\n",
      "0.0\n",
      "2.337320616055983\n",
      "10.501493542296416\n",
      "10.619664352674096\n",
      "41.185890431015146\n",
      "16.38954776253876\n",
      "10.619661791353158\n",
      "0.2247431057710383\n",
      "13.099814576280504\n",
      "22.24903983921941\n",
      "20.556052802479396\n",
      "17.38057534082223\n",
      "14.036224867031086\n",
      "7.06380383774199\n",
      "17.727621948305632\n",
      "15.255169454615533\n",
      "1.5074808064297596\n",
      "0.27552897734466875\n",
      "7.33431006472576\n",
      "3.814075816504439\n",
      "0.0\n",
      "12.291359982223888\n",
      "11.523703175296813\n",
      "29.981578107087238\n",
      "8.714729288912595\n",
      "10.704736706664132\n",
      "8.012833009705874\n",
      "22.166389427620377\n",
      "0.0\n",
      "12.704005434043385\n",
      "9.201093399330862\n",
      "4.719360775124712\n",
      "4.948344414514124\n",
      "10.773617670339828\n",
      "2.792716648777412\n",
      "23.962550740641664\n",
      "0.16999956646021008\n",
      "6.188607303158651\n",
      "19.279335166522102\n",
      "13.351670011240206\n",
      "20.998745323674836\n",
      "13.834852107278891\n",
      "13.240536003254928\n",
      "22.53015249621056\n",
      "12.339071414897365\n",
      "8.367842763987865\n",
      "11.88865800517546\n",
      "19.440012698704006\n",
      "11.663699748445564\n",
      "7.125014285536351\n",
      "8.325674029825851\n",
      "21.614774812513687\n",
      "10.918510315184719\n",
      "23.280686979383766\n",
      "0.7073224376154723\n",
      "4.3542455959055815e-05\n",
      "4.0856048643164184\n",
      "9.2725704752214\n",
      "10.885523489701319\n",
      "11.003557696295601\n",
      "10.619635324370124\n",
      "0.0\n",
      "11.450273839242415\n",
      "19.29006443321284\n",
      "5.305137701943133\n",
      "5.923050402002202\n",
      "15.14387621784523\n",
      "45.58465391924795\n",
      "7.594670899471122\n",
      "9.05786902819075\n",
      "1.0416192163525775\n",
      "1.0416192163525775\n",
      "18.434948654789338\n",
      "5.606128130409729\n",
      "8.595915160092462\n",
      "14.318761191577364\n",
      "4.573896795627922\n",
      "8.194267380053741\n",
      "4.05854963124035\n",
      "17.939093196106544\n",
      "12.804273037945995\n",
      "0.470638880514783\n",
      "2.862376467461903\n",
      "3.4336340662913174\n",
      "6.724329775614017\n",
      "13.262971040244302\n",
      "21.392141381422533\n",
      "8.972611191876066\n",
      "17.499588398901942\n",
      "30.934894289972362\n",
      "9.246126117189228\n",
      "3.8014709158345923\n",
      "6.911203324418741\n",
      "4.643299628444892\n",
      "25.68365766619462\n",
      "10.61962251776543\n",
      "3.189640285788009\n",
      "19.805322164810963\n",
      "14.334624732811543\n",
      "13.016117331469463\n",
      "9.599783494015554\n",
      "19.65384201595501\n",
      "3.1254399226844747\n",
      "16.14432988407014\n",
      "14.03625133401412\n",
      "11.473486237136704\n",
      "26.54056636927525\n",
      "1.169116436598501\n",
      "1.169116436598501\n",
      "0.18075540685568647\n",
      "13.893238271851454\n",
      "12.886845329256396\n",
      "11.592185960288239\n",
      "8.806810911094944\n",
      "5.804434988998232\n",
      "7.316898204984103\n",
      "8.130099673792177\n",
      "33.31062767647709\n",
      "5.931520476903252\n",
      "3.6913975081613706\n",
      "26.565003078578265\n",
      "14.058958297909829\n",
      "17.35271286476363\n",
      "10.245889080534319\n",
      "1.6090158373102295\n",
      "45.784918053920485\n",
      "13.080370628539065\n",
      "23.51455180279136\n",
      "12.027803045833782\n",
      "4.853866249706745\n",
      "4.763684273887305\n",
      "0.0\n",
      "13.633988196619864\n",
      "[]\n",
      "-0.25106403312061376\n",
      "-3.103566454785743\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 주어진 Train Dataset 경로.\n",
    "    train_path = '/mnt/hackerton/dataset/Dataset/Train/spine/'\n",
    "    image_size = 512\n",
    "    epochs = 100\n",
    "    batch_sizes = 8\n",
    "    # check, pred 저장할 개인서버 저장경로.\n",
    "    path = '/home/hackerton/jupyter/Spine_data/'\n",
    "    # 주어진 Train Dataset 경로.(Train Dataset의 일부를 Test Dataset으로 씀.)\n",
    "    test_path = '/mnt/hackerton/dataset/Dataset/Train/spine/'\n",
    "    \n",
    "    imgs_train, imgs_mask_train, imgs_name = train_data_loading(train_path, image_size = image_size)\n",
    "    model = deep(imgs_train, imgs_mask_train, path, batch_size = batch_sizes, epochs = epochs, image_size=image_size)\n",
    "\n",
    "# 학습 안하고 가중치 파일을 이용해 평가시 위에 2줄 주석처리. 아래 코드 주석 해제.\n",
    "#     from tensorflow.keras import Model\n",
    "#     model = hourglass_network(input_shape=(512, 512, 1), num_classes=16, num_stacks=1, num_channels=32)\n",
    "#     model.compile(optimizer=Adam(lr=0.001), loss=dice_coef_loss, metrics=['accuracy', sens, dice_coef_loss])\n",
    "#     model.load_weights('/home/hackerton/jupyter/Spine_data/A_pred/j1.h5')\n",
    "    \n",
    "    test_name = predict_val(model, test_path, image_size = image_size)\n",
    "    angle_list,dist_list = get_results(test_name)\n",
    "    get_score(angle_list, dist_list, test_name, test_path)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
