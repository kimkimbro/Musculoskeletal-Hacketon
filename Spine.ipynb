{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7164d520",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydicom'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10388/812365870.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpydicom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pydicom'"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import pydicom\n",
    "import cv2\n",
    "import json\n",
    "import pandas as pd\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import pydicom as dicom\n",
    "import sys\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import LinearRing\n",
    "from skimage import draw\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import *\n",
    "from keras.layers import Input, merge, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Cropping2D, concatenate, Activation,Conv2DTranspose\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, History, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import backend as keras\n",
    "from keras.utils.multi_gpu_utils import multi_gpu_model\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111fa291",
   "metadata": {},
   "source": [
    "# Data Prepare\n",
    "\n",
    "## 구조\n",
    "\n",
    "### .dcm  \n",
    "#### ----(data_pre함수)----   : data 나눔\n",
    "### .jpg(x-ray image) , .mask(척추 image)  저장\n",
    "#### ----(augmentation함수)----      :   data 수 늘림 및 aug 처리 \n",
    "### .jpg(x-ray image) , .mask(척추 image) 저장 \n",
    "#### ----(create_train_data 함수)----  :  데이터 표준화 및 scale 맞춤\n",
    "### .jpg(x-ray image) , .mask(척추 image), img_name 반환 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ab7ca0",
   "metadata": {},
   "source": [
    "## 0. 전처리 필요 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172fa19e",
   "metadata": {},
   "source": [
    "1) zero padding\n",
    "* 무조건 정사각형으로 바꿈 ( 작은거에 zero padding 해서 label 값에는 지장을 안줌)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3762f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(img):\n",
    "    y, x = img.shape\n",
    "    if x > y:\n",
    "        new_size = x\n",
    "        add_size = x-y\n",
    "        \n",
    "        add_image = np.zeros((new_size, add_size))\n",
    "        if add_size % 2==0:\n",
    "            add_img = np.zeros((int(add_size/2), new_size))\n",
    "            img = np.concatenate((add_img,img, add_img),axis=0)\n",
    "\n",
    "        elif add_size % 2==1:\n",
    "            add_img_top = np.zeros((int(add_size/2),new_size))\n",
    "            add_img_bot = np.zeros((int(add_size/2)+1, new_size))\n",
    "            img = np.concatenate((add_img_top,img, add_img_bot),axis=0)\n",
    "\n",
    "\n",
    "    elif y > x:\n",
    "        new_size = y\n",
    "        add_size = y-x\n",
    "        \n",
    "        add_image = np.zeros((add_size, new_size))\n",
    "        if add_size % 2==0:\n",
    "            add_img = np.zeros((new_size, int(add_size/2)))\n",
    "            img = np.concatenate((add_img,img, add_img),axis=1)\n",
    "\n",
    "        elif add_size % 2==1:\n",
    "            add_img_left = np.zeros((new_size, int(add_size/2)))\n",
    "            add_img_right = np.zeros((new_size, int(add_size/2)+1))\n",
    "            img = np.concatenate((add_img_left,img, add_img_right),axis=1)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b45ea67",
   "metadata": {},
   "source": [
    "2) poly mask\n",
    "* 좌표 값들을 받아 도형의 outline 을 그린 후 outline 안은 1 밖은 0 처리\n",
    "* vertex_row_coords ,vertex_row_coords -> 도형 outline이라 생각\n",
    "    \n",
    "    out line 밖은 0(검은색) 안은 1(흰색) 바꿈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aa0c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly2mask(vertex_row_coords, vertex_col_coords, shape):\n",
    "    fill_row_coords, fill_col_coords = draw.polygon(\n",
    "        vertex_row_coords, vertex_col_coords, shape)\n",
    "    mask = np.zeros(shape, dtype=np.bool)\n",
    "    mask[fill_row_coords, fill_col_coords] = True\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2763829d",
   "metadata": {},
   "source": [
    "3) mkfolder\n",
    "\n",
    "*경로에 폴더가 없으면 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe083b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkfolder(folder):\n",
    "    if not os.path.lexists(folder):\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a653d5",
   "metadata": {},
   "source": [
    "4) data augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b323fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_center(img):\n",
    "    contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "    cnt = contours[0] \n",
    "    mmt = cv2.moments(cnt) \n",
    "    cx = int(mmt['m10']/mmt['m00']) \n",
    "    cy = int(mmt['m01']/mmt['m00']) \n",
    "    print( 'x 무게중심', cx, 'y 무게중심', cy )\n",
    "    return cx, cy\n",
    "\n",
    "def randomRoate(img, label, p, angle_range):\n",
    "    if len(img.shape)==3:\n",
    "        height, width, channel = img.shape\n",
    "    else:\n",
    "        height, width = img.shape\n",
    "    \n",
    "    random_angle = np.random.randint(-angle_range/2, angle_range/2)*2\n",
    "    matrix = cv2.getRotationMatrix2D((width/2, height/2), random_angle, 1)\n",
    "    rotate_img = cv2.warpAffine(img, matrix, (width, height))\n",
    "    rotate_label = cv2.warpAffine(label, matrix, (width, height))\n",
    "    \n",
    "    return rotate_img, rotate_label\n",
    "\n",
    "def find_top_point(ori_img, mask_img):\n",
    "    mask_img_T = mask_img.T\n",
    "\n",
    "    tmp_index=[]\n",
    "    for i, c in enumerate(mask_img_T):\n",
    "        if len(np.unique(c))>1:\n",
    "            tmp_index.append(i)\n",
    "    x_min = min(tmp_index)\n",
    "    x_max = max(tmp_index)\n",
    "\n",
    "    p_x = int((x_min+x_max)/2) \n",
    "    p_y = np.where(mask_img_T[int((x_min+x_max)/2)]==255)[0][0]\n",
    "    p = (p_x, p_y)\n",
    "\n",
    "    return p_x, p_y\n",
    "\n",
    "def Augment_crop(img, mask): # 이미지에서 특정 부분 추출\n",
    "    p_x, p_y=find_top_point(img, mask)\n",
    "    rotate_img, rotate_mask = randomRoate(img, mask, (p_x, p_y), 20)\n",
    "    random_size = np.random.randint(15,35)*20 # 300-600\n",
    "    \n",
    "    h, w= img.shape\n",
    "    x1 = p_x-random_size if p_x-random_size>0 else 0\n",
    "    x2 = p_x+random_size if p_x+random_size<w else w\n",
    "    y1 = p_y-random_size if p_y-random_size>0 else 0\n",
    "    y2 = p_y+random_size if p_y+random_size<h else h\n",
    "    \n",
    "    crop_img = rotate_img[y1:y2,x1:x2]\n",
    "    crop_mask = rotate_mask[y1:y2,x1:x2]\n",
    "\n",
    "    return crop_img, crop_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3055f5e8",
   "metadata": {},
   "source": [
    "## 1. 이미지 전처리\n",
    "\n",
    "### 1) 이미지 변환 및 mask 파일 생성\n",
    "* 이미지 데이터 불러와서 1 표준화(이때 표준화는 0 ~255) 2 resize \n",
    "* json 데이터 불러와서 3. 지정 좌표(5개 척추 들)의 mask 만듬 (크기 맞춤) \n",
    "\n",
    "#### 내가 바꿀 수 있는거 \n",
    "1.  선명도 추가 \n",
    "2. resize방법 변경\n",
    "3. 표준화를 여기 구지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a19a84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pre(dcmlist):\n",
    "    for i in range(len(dcmlist)):\n",
    "        # try:\n",
    "        dicompath = dcmlist[i]\n",
    "        dicom = pydicom.read_file(dicompath)\n",
    "        img = dicom.pixel_array\n",
    "        img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        # except:\n",
    "        #     print(dicompath)\n",
    "        #     continue\n",
    "        pad_img = zero_padding(img)\n",
    "        image = Image.fromarray(pad_img)\n",
    "        image = image.convert('L')\n",
    "        image.save(dicompath.replace('.dcm','.jpg'))\n",
    "    \n",
    "    for dcm in dcmlist:\n",
    "        jsonfile = dcm[:-4]+'.json'\n",
    "        reader = sitk.ReadImage(dcm)\n",
    "        image_array = sitk.GetArrayFromImage(reader) # 왜 써놓은 거임\n",
    "        height = reader.GetMetaData('0028|0010')\n",
    "        width = reader.GetMetaData('0028|0011')\n",
    "        data = []\n",
    "        for line in open(jsonfile,'r'):\n",
    "            data.append(json.loads(line))\n",
    "        for json_data in data:\n",
    "            mask = np.zeros((int(height), int(width)))\n",
    "            if json_data['annotation']['ANNOTATION_DATA'] is not None:\n",
    "                for m in json_data['annotation']['ANNOTATION_DATA']:\n",
    "                    if 'm_points' in m:\n",
    "                        a = []\n",
    "                        for i in m['m_points']:\n",
    "                            b = (i['x'], i['y'])\n",
    "                            a.append(b)\n",
    "                        r = LinearRing(a)\n",
    "                        s = Polygon(r)\n",
    "                        x, y = s.exterior.coords.xy\n",
    "                        maskd = poly2mask(y, x, (int(height), int(width)))\n",
    "                        mask = mask + maskd\n",
    "                mask = mask*255\n",
    "                mask = zero_padding(mask)\n",
    "                mask = np.expand_dims(mask, axis=0)\n",
    "                img = sitk.GetImageFromArray(mask.astype('uint8'))\n",
    "                num = 0 # 왜쓰는거임\n",
    "                maskpath = dcm[:-4]+'.png'\n",
    "                sitk.WriteImage(img, maskpath)\n",
    "            else:\n",
    "                print('json data is incorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77de853",
   "metadata": {},
   "source": [
    "### 2) 이미지 augmentation\n",
    "\n",
    "#### 내가 바꿀 수 있는거 \n",
    "1.  aug 방법 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73c37b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(img_path,mask_path): \n",
    "    img_li = sorted(glob.glob(img_path))\n",
    "    mask_li = sorted(glob.glob(mask_path))\n",
    "    print(len(img_li), len(mask_li))\n",
    "    i=0\n",
    "\n",
    "    for img, mask in zip(img_li, mask_li):\n",
    "        if i%100==0:\n",
    "            print('{}/{}'.format(i, len(img_li)))\n",
    "        savepath = 'train/aug/'\n",
    "        mkfolder(savepath)\n",
    "        ori_img = cv2.imread(img, 0)\n",
    "        mask_img = cv2.imread(mask, 0)\n",
    "        img_name = img[img.rindex('/')+1:-4]\n",
    "        mask_name = img[mask.rindex('/')+1:-4]\n",
    "        print(img_name)\n",
    "        print(mask_name)\n",
    "        cv2.imwrite(savepath+'/{}.jpg'.format(img_name), cv2.resize(ori_img, (512,512)))\n",
    "        cv2.imwrite(savepath+'/{}.png'.format(img_name), cv2.resize(mask_img, (512,512)))\n",
    "        for j in range(9):\n",
    "            aug_img, aug_mask = Augment_crop(ori_img, mask_img)        \n",
    "            aug_img = cv2.resize(aug_img, (512,512))\n",
    "            aug_mask = cv2.resize(aug_mask, (512,512))\n",
    "            cv2.imwrite(savepath+'/{}_{}.jpg'.format(img_name, j), aug_img)\n",
    "            cv2.imwrite(savepath+'/{}_{}.png'.format(img_name, j), aug_mask)\n",
    "        i+=1\n",
    "        \n",
    "    return savepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e456ca",
   "metadata": {},
   "source": [
    "## 2. create train test dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b75daf",
   "metadata": {},
   "source": [
    "### 1) train data 생성 \n",
    "* 원하는 channel 및 size로 바꿈\n",
    "* 표준화 진행(0~1) \n",
    "#### 내가 바꿀 수 있는거 \n",
    "1. resize방법 변경\n",
    "2. 표준화 방법 변경\n",
    "3. 선명도 처리 추가\n",
    "4. channel 확장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cba4da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data(train_path, out_rows, out_cols, name, img_type):\n",
    "    print('-'*30)\n",
    "    print('Creating training images...')\n",
    "    print('-'*30)\n",
    "    \n",
    "    imgs = glob.glob(train_path +\"*.\" + img_type)\n",
    "    imgdatas = np.ndarray((len(imgs),out_rows,out_cols,1), dtype=np.uint8)\n",
    "    imglabels = np.ndarray((len(imgs),out_rows,out_cols,1), dtype=np.uint8)\n",
    "    imgnames=[]\n",
    "    for i, imgname in enumerate(imgs):\n",
    "        if i%1000==0:\n",
    "            print('{}/{}'.format(i, len(imgs)))\n",
    "        midname = imgname[imgname.rindex(\"/\")+1:-4]     \n",
    "        img = load_img(imgname, color_mode = \"grayscale\")\n",
    "        label = load_img(imgname.replace('jpg', 'png'), color_mode = \"grayscale\")\n",
    "        img=img.resize((out_rows,out_cols))\n",
    "        label=label.resize((out_rows,out_cols))\n",
    "\n",
    "        img = img_to_array(img)\n",
    "        label = img_to_array(label)\n",
    "        imgdatas[i] = img\n",
    "        imglabels[i] = label\n",
    "        imgnames.append(midname)\n",
    "    \n",
    "    imgdatas = imgdatas.astype('uint8')\n",
    "    imglabels = imglabels.astype('uint8')\n",
    "    \n",
    "    print('img : ', imgdatas.max())\n",
    "    print('mask : ',imglabels.max())\n",
    "    \n",
    "    print('-'*30)\n",
    "    print('normalization start...')\n",
    "    print('-'*30)\n",
    "    imgdatas = imgdatas/255.0\n",
    "\n",
    "    imglabels[imglabels <= 127] = 0\n",
    "    imglabels[imglabels > 127] = 1\n",
    "\n",
    "    print('img : ',imgdatas.max())\n",
    "    print('mask : ',imglabels.max())\n",
    "    print('mask : ',imglabels.min())\n",
    "    \n",
    "    print('loading done')\n",
    "    return(imgdatas,imglabels,imgnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6d2b4a",
   "metadata": {},
   "source": [
    "### 2) test data 생성 \n",
    "\n",
    "#### 내가 바꿀 수 있는거 \n",
    "1. resize방법 변경\n",
    "2. 표준화 방법 변경\n",
    "3. 선명도 처리 추가\n",
    "4. channel 확장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3fea05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data(test_path, out_rows, out_cols, name, img_type):\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Creating test images...')\n",
    "    print('-'*30)\n",
    "\n",
    "    i = 0\n",
    "    imgs = glob.glob(test_path + \"*.\" + img_type)\n",
    "    imgdatas = np.ndarray((len(imgs),out_rows,out_cols,1), dtype=np.uint8)\n",
    "    imglabels = np.ndarray((len(imgs),out_rows,out_cols,1), dtype=np.uint8)\n",
    "    \n",
    "    imgnames=[]\n",
    "    for j, imgname in enumerate(imgs):\n",
    "        if j%100==0:\n",
    "            print('{}/{}'.format(j, len(imgs)))\n",
    "        midname = imgname[imgname.rindex(\"/\")+1:-4]\n",
    "        img = load_img(imgname, color_mode = \"grayscale\")\n",
    "        label = load_img(imgname.replace('jpg', 'png'), color_mode = \"grayscale\")\n",
    "        img=img.resize((out_rows,out_cols))\n",
    "        label=label.resize((out_rows,out_cols))\n",
    "\n",
    "        img = img_to_array(img)\n",
    "        label = img_to_array(label)\n",
    "        imgdatas[j] = img\n",
    "        imglabels[j] = label\n",
    "        imgnames.append(midname)\n",
    "         \n",
    "    imgdatas = imgdatas.astype('uint8')\n",
    "    imglabels = imglabels.astype('uint8')\n",
    "    \n",
    "    print('img : ', imgdatas.max())\n",
    "    print('mask : ',imglabels.max())\n",
    "    \n",
    "    print('-'*30)\n",
    "    print('normalization start...')\n",
    "    print('-'*30)\n",
    "    imgdatas = imgdatas/255.0\n",
    "    \n",
    "    imglabels[imglabels <= 127] = 0\n",
    "    imglabels[imglabels > 127] = 1\n",
    "    \n",
    "    print('img : ',imgdatas.max())\n",
    "    print('mask : ',imglabels.max())\n",
    "    print('mask : ',imglabels.min())\n",
    "    print('loading done')\n",
    "    return(imgdatas,imglabels,imgnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe9dc55",
   "metadata": {},
   "source": [
    "## 3. train, test data Load ( 다 합친거)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fc105f",
   "metadata": {},
   "source": [
    "### 1) train data\n",
    "* 저장된 data 를 받아서 -> (batch, h, w, 1)의 np.array로 return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddce9a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_loading(path,image_size = 512):\n",
    "    mkfolder(path)\n",
    "    trainlist = glob.glob(path+'/*.dcm')\n",
    "    data_pre(trainlist)\n",
    "    train_img_path = path+'/*.jpg'\n",
    "    train_mask_path = path+'/*.png'\n",
    "    aug_path = augmentation(train_img_path,train_mask_path)\n",
    "    imgs_train, imgs_mask_train, imgs_name = create_train_data(aug_path, image_size, image_size, 'train', 'jpg')\n",
    "    return imgs_train, imgs_mask_train, imgs_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730edb7",
   "metadata": {},
   "source": [
    "# 전체코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a491cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import pydicom\n",
    "import cv2\n",
    "import json\n",
    "import pandas as pd\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import pydicom as dicom\n",
    "import sys\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import LinearRing\n",
    "from skimage import draw\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import *\n",
    "from keras.layers import Input, merge, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Cropping2D, concatenate, Activation,Conv2DTranspose\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, History, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import backend as keras\n",
    "from keras.utils.multi_gpu_utils import multi_gpu_model\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "def zero_padding(img):\n",
    "    y, x = img.shape\n",
    "    if x > y:\n",
    "        new_size = x\n",
    "        add_size = x-y\n",
    "        \n",
    "        add_image = np.zeros((new_size, add_size))\n",
    "        if add_size % 2==0:\n",
    "            add_img = np.zeros((int(add_size/2), new_size))\n",
    "            img = np.concatenate((add_img,img, add_img),axis=0)\n",
    "\n",
    "        elif add_size % 2==1:\n",
    "            add_img_top = np.zeros((int(add_size/2),new_size))\n",
    "            add_img_bot = np.zeros((int(add_size/2)+1, new_size))\n",
    "            img = np.concatenate((add_img_top,img, add_img_bot),axis=0)\n",
    "\n",
    "\n",
    "    elif y > x:\n",
    "        new_size = y\n",
    "        add_size = y-x\n",
    "        \n",
    "        add_image = np.zeros((add_size, new_size))\n",
    "        if add_size % 2==0:\n",
    "            add_img = np.zeros((new_size, int(add_size/2)))\n",
    "            img = np.concatenate((add_img,img, add_img),axis=1)\n",
    "\n",
    "        elif add_size % 2==1:\n",
    "            add_img_left = np.zeros((new_size, int(add_size/2)))\n",
    "            add_img_right = np.zeros((new_size, int(add_size/2)+1))\n",
    "            img = np.concatenate((add_img_left,img, add_img_right),axis=1)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def poly2mask(vertex_row_coords, vertex_col_coords, shape):\n",
    "    fill_row_coords, fill_col_coords = draw.polygon(\n",
    "        vertex_row_coords, vertex_col_coords, shape)\n",
    "    mask = np.zeros(shape, dtype=np.bool)\n",
    "    mask[fill_row_coords, fill_col_coords] = True\n",
    "    return mask\n",
    "\n",
    "def mkfolder(folder):\n",
    "    if not os.path.lexists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "def create_train_data(train_path, out_rows, out_cols, name, img_type):\n",
    "    print('-'*30)\n",
    "    print('Creating training images...')\n",
    "    print('-'*30)\n",
    "    \n",
    "    imgs = glob.glob(train_path +\"*.\" + img_type)\n",
    "    imgdatas = np.ndarray((len(imgs),out_rows,out_cols,1), dtype=np.uint8)\n",
    "    imglabels = np.ndarray((len(imgs),out_rows,out_cols,1), dtype=np.uint8)\n",
    "    imgnames=[]\n",
    "    for i, imgname in enumerate(imgs):\n",
    "        if i%1000==0:\n",
    "            print('{}/{}'.format(i, len(imgs)))\n",
    "        midname = imgname[imgname.rindex(\"/\")+1:-4]     \n",
    "        img = load_img(imgname, color_mode = \"grayscale\")\n",
    "        label = load_img(imgname.replace('jpg', 'png'), color_mode = \"grayscale\")\n",
    "        img=img.resize((out_rows,out_cols))\n",
    "        label=label.resize((out_rows,out_cols))\n",
    "\n",
    "        img = img_to_array(img)\n",
    "        label = img_to_array(label)\n",
    "        imgdatas[i] = img\n",
    "        imglabels[i] = label\n",
    "        imgnames.append(midname)\n",
    "    \n",
    "    imgdatas = imgdatas.astype('uint8')\n",
    "    imglabels = imglabels.astype('uint8')\n",
    "    \n",
    "    print('img : ', imgdatas.max())\n",
    "    print('mask : ',imglabels.max())\n",
    "    \n",
    "    print('-'*30)\n",
    "    print('normalization start...')\n",
    "    print('-'*30)\n",
    "    imgdatas = imgdatas/255.0\n",
    "    \n",
    "    imglabels[imglabels <= 127] = 0\n",
    "    imglabels[imglabels > 127] = 1\n",
    "    \n",
    "    print('img : ',imgdatas.max())\n",
    "    print('mask : ',imglabels.max())\n",
    "    print('mask : ',imglabels.min())\n",
    "    \n",
    "    print('loading done')\n",
    "    return(imgdatas,imglabels,imgnames)\n",
    "    \n",
    "def create_test_data(test_path, out_rows, out_cols, name, img_type):\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Creating test images...')\n",
    "    print('-'*30)\n",
    "\n",
    "    i = 0\n",
    "    imgs = glob.glob(test_path + \"*.\" + img_type)\n",
    "    imgdatas = np.ndarray((len(imgs),out_rows,out_cols,1), dtype=np.uint8)\n",
    "    imglabels = np.ndarray((len(imgs),out_rows,out_cols,1), dtype=np.uint8)\n",
    "    \n",
    "    imgnames=[]\n",
    "    for j, imgname in enumerate(imgs):\n",
    "        if j%100==0:\n",
    "            print('{}/{}'.format(j, len(imgs)))\n",
    "        midname = imgname[imgname.rindex(\"/\")+1:-4]\n",
    "        img = load_img(imgname, color_mode = \"grayscale\")\n",
    "        label = load_img(imgname.replace('jpg', 'png'), color_mode = \"grayscale\")\n",
    "        img=img.resize((out_rows,out_cols))\n",
    "        label=label.resize((out_rows,out_cols))\n",
    "\n",
    "        img = img_to_array(img)\n",
    "        label = img_to_array(label)\n",
    "        imgdatas[j] = img\n",
    "        imglabels[j] = label\n",
    "        imgnames.append(midname)\n",
    "         \n",
    "    imgdatas = imgdatas.astype('uint8')\n",
    "    imglabels = imglabels.astype('uint8')\n",
    "    \n",
    "    print('img : ', imgdatas.max())\n",
    "    print('mask : ',imglabels.max())\n",
    "    \n",
    "    print('-'*30)\n",
    "    print('normalization start...')\n",
    "    print('-'*30)\n",
    "    imgdatas = imgdatas/255.0\n",
    "    \n",
    "    imglabels[imglabels <= 127] = 0\n",
    "    imglabels[imglabels > 127] = 1\n",
    "    \n",
    "    print('img : ',imgdatas.max())\n",
    "    print('mask : ',imglabels.max())\n",
    "    print('mask : ',imglabels.min())\n",
    "    print('loading done')\n",
    "    return(imgdatas,imglabels,imgnames)\n",
    "\n",
    "def weight_center(img):\n",
    "    contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "    cnt = contours[0] \n",
    "    mmt = cv2.moments(cnt) \n",
    "    cx = int(mmt['m10']/mmt['m00']) \n",
    "    cy = int(mmt['m01']/mmt['m00']) \n",
    "    print( 'x 무게중심', cx, 'y 무게중심', cy )\n",
    "    return cx, cy\n",
    "def randomRoate(img, label, p, angle_range):\n",
    "    if len(img.shape)==3:\n",
    "        height, width, channel = img.shape\n",
    "    else:\n",
    "        height, width = img.shape\n",
    "    \n",
    "    random_angle = np.random.randint(-angle_range/2, angle_range/2)*2\n",
    "    matrix = cv2.getRotationMatrix2D((width/2, height/2), random_angle, 1)\n",
    "    rotate_img = cv2.warpAffine(img, matrix, (width, height))\n",
    "    rotate_label = cv2.warpAffine(label, matrix, (width, height))\n",
    "    \n",
    "    return rotate_img, rotate_label\n",
    "\n",
    "def find_top_point(ori_img, mask_img):\n",
    "    mask_img_T = mask_img.T\n",
    "\n",
    "    tmp_index=[]\n",
    "    for i, c in enumerate(mask_img_T):\n",
    "        if len(np.unique(c))>1:\n",
    "            tmp_index.append(i)\n",
    "    x_min = min(tmp_index)\n",
    "    x_max = max(tmp_index)\n",
    "\n",
    "    p_x = int((x_min+x_max)/2) \n",
    "    p_y = np.where(mask_img_T[int((x_min+x_max)/2)]==255)[0][0]\n",
    "    p = (p_x, p_y)\n",
    "\n",
    "    return p_x, p_y\n",
    "\n",
    "def Augment_crop(img, mask):\n",
    "    p_x, p_y=find_top_point(img, mask)\n",
    "    rotate_img, rotate_mask = randomRoate(img, mask, (p_x, p_y), 20)\n",
    "    random_size = np.random.randint(15,35)*20 # 300-600\n",
    "    \n",
    "    h, w= img.shape\n",
    "    x1 = p_x-random_size if p_x-random_size>0 else 0\n",
    "    x2 = p_x+random_size if p_x+random_size<w else w\n",
    "    y1 = p_y-random_size if p_y-random_size>0 else 0\n",
    "    y2 = p_y+random_size if p_y+random_size<h else h\n",
    "    \n",
    "    crop_img = rotate_img[y1:y2,x1:x2]\n",
    "    crop_mask = rotate_mask[y1:y2,x1:x2]\n",
    "\n",
    "    return crop_img, crop_mask\n",
    "\n",
    "def data_pre(dcmlist):\n",
    "    for i in range(len(dcmlist)):\n",
    "        # try:\n",
    "        dicompath = dcmlist[i]\n",
    "        dicom = pydicom.read_file(dicompath)\n",
    "        img = dicom.pixel_array\n",
    "        img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        # except:\n",
    "        #     print(dicompath)\n",
    "        #     continue\n",
    "        pad_img = zero_padding(img)\n",
    "        image = Image.fromarray(pad_img)\n",
    "        image = image.convert('L')\n",
    "        image.save(dicompath.replace('.dcm','.jpg'))\n",
    "    \n",
    "    for dcm in dcmlist:\n",
    "        jsonfile = dcm[:-4]+'.json'\n",
    "        reader = sitk.ReadImage(dcm)\n",
    "        image_array = sitk.GetArrayFromImage(reader)\n",
    "        height = reader.GetMetaData('0028|0010')\n",
    "        width = reader.GetMetaData('0028|0011')\n",
    "        data = []\n",
    "        for line in open(jsonfile,'r'):\n",
    "            data.append(json.loads(line))\n",
    "        for json_data in data:\n",
    "            mask = np.zeros((int(height), int(width)))\n",
    "            if json_data['annotation']['ANNOTATION_DATA'] is not None:\n",
    "                for m in json_data['annotation']['ANNOTATION_DATA']:\n",
    "                    if 'm_points' in m:\n",
    "                        a = []\n",
    "                        for i in m['m_points']:\n",
    "                            b = (i['x'], i['y'])\n",
    "                            a.append(b)\n",
    "                        r = LinearRing(a)\n",
    "                        s = Polygon(r)\n",
    "                        x, y = s.exterior.coords.xy\n",
    "                        maskd = poly2mask(y, x, (int(height), int(width)))\n",
    "                        mask = mask + maskd\n",
    "                mask = mask*255\n",
    "                mask = zero_padding(mask)\n",
    "                mask = np.expand_dims(mask, axis=0)\n",
    "                img = sitk.GetImageFromArray(mask.astype('uint8'))\n",
    "                num = 0\n",
    "                maskpath = dcm[:-4]+'.png'\n",
    "                sitk.WriteImage(img, maskpath)\n",
    "            else:\n",
    "                print('haha')\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "def augmentation(img_path,mask_path):\n",
    "    img_li = sorted(glob.glob(img_path))\n",
    "    mask_li = sorted(glob.glob(mask_path))\n",
    "    print(len(img_li), len(mask_li))\n",
    "    i=0\n",
    "\n",
    "    for img, mask in zip(img_li, mask_li):\n",
    "        if i%100==0:\n",
    "            print('{}/{}'.format(i, len(img_li)))\n",
    "        savepath = 'train/aug/'\n",
    "        mkfolder(savepath)\n",
    "        ori_img = cv2.imread(img, 0)\n",
    "        mask_img = cv2.imread(mask, 0)\n",
    "        img_name = img[img.rindex('/')+1:-4]\n",
    "        mask_name = img[mask.rindex('/')+1:-4]\n",
    "        print(img_name)\n",
    "        print(mask_name)\n",
    "        cv2.imwrite(savepath+'/{}.jpg'.format(img_name), cv2.resize(ori_img, (512,512)))\n",
    "        cv2.imwrite(savepath+'/{}.png'.format(img_name), cv2.resize(mask_img, (512,512)))\n",
    "        for j in range(9):\n",
    "            aug_img, aug_mask = Augment_crop(ori_img, mask_img)        \n",
    "            aug_img = cv2.resize(aug_img, (512,512))\n",
    "            aug_mask = cv2.resize(aug_mask, (512,512))\n",
    "            cv2.imwrite(savepath+'/{}_{}.jpg'.format(img_name, j), aug_img)\n",
    "            cv2.imwrite(savepath+'/{}_{}.png'.format(img_name, j), aug_mask)\n",
    "        i+=1\n",
    "    return savepath\n",
    "\n",
    "def train_data_loading(path,image_size = 512):\n",
    "    mkfolder(path)\n",
    "    trainlist = glob.glob(path+'/*.dcm')\n",
    "    data_pre(trainlist)\n",
    "    train_img_path = path+'/*.jpg'\n",
    "    train_mask_path = path+'/*.png'\n",
    "    aug_path = augmentation(train_img_path,train_mask_path)\n",
    "    imgs_train, imgs_mask_train, imgs_name = create_train_data(aug_path, image_size, image_size, 'train', 'jpg')\n",
    "    return imgs_train, imgs_mask_train, imgs_name\n",
    "\n",
    "\n",
    "def get_unet(img_rows, img_cols):\n",
    "    inputs = Input((img_rows, img_cols,1))\n",
    "    conv1 = Conv2D(32, (3, 3), activation=None, padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), activation=None, padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation=None, padding='same')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), activation=None, padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation=None, padding='same')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), activation=None, padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation=None, padding='same')(pool3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    conv4 = Conv2D(256, (3, 3), activation=None, padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation=None, padding='same')(pool4)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5 = Conv2D(512, (3, 3), activation=None, padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation=None, padding='same')(up6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Activation('relu')(conv6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation=None, padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Activation('relu')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation=None, padding='same')(up7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation=None, padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation=None, padding='same')(up8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Activation('relu')(conv8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation=None, padding='same')(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Activation('relu')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation=None, padding='same')(up9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation=None, padding='same')(conv9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    return model\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1e-6):\n",
    "    print(y_pred)\n",
    "    print(y_true)\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)\n",
    "\n",
    "def sens(y_true, y_pred): # sensitivity, recall\n",
    "    print(y_pred)\n",
    "    print(y_true)\n",
    "    y_target_yn = K.round(K.clip(y_true, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "def sch(epoch):\n",
    "    if epoch>100 and epoch<=250:\n",
    "        return 0.0001\n",
    "    elif epoch>250:\n",
    "        return 0.00001\n",
    "    else:\n",
    "        return 0.001\n",
    "\n",
    "\n",
    "def deep(imgs_train,imgs_mask_train,path,batch_size = 4,epochs = 10,image_size=512):\n",
    "    model = get_unet(image_size, image_size)\n",
    "    model.summary()\n",
    "    model = multi_gpu_model(model,gpus=2)\n",
    "    model.compile(optimizer=Adam(lr=0.0001), loss=dice_coef_loss, \n",
    "                    metrics=['accuracy', sens, dice_coef_loss])\n",
    "    \n",
    "    check_model_path = path+'check/'\n",
    "    predict_path = path+'pred/'\n",
    "    mkfolder(check_model_path)\n",
    "    mkfolder(predict_path)\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(check_model_path+'ap_aug_exp1_{epoch:d}_{loss:f}.hdf5', \n",
    "                                        monitor='val_dice_coef_loss',verbose=1, \n",
    "                                        save_best_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_dice_coef_loss', factor=0.8, min_delta = 0.01, \n",
    "                                    patience=5, min_lr=1e-6, verbose=1)\n",
    "    earlystopping = EarlyStopping(monitor='val_dice_coef_loss', patience=10)\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "\n",
    "    print('Fitting model...')\n",
    "    model.fit(imgs_train, imgs_mask_train, batch_size=batch_size, epochs=epochs, verbose=1, \n",
    "              validation_split=0.2, shuffle=True, callbacks=[model_checkpoint, earlystopping])\n",
    "\n",
    "    print('save model')\n",
    "    model.save(predict_path+'ap_aug_exp1.h5')\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_save(pred_list,name_list):\n",
    "    pred_img_path = 'train/pred/'\n",
    "    if not os.path.isdir(pred_img_path):\n",
    "        os.makedirs(pred_img_path)\n",
    "\n",
    "    imgs = pred_list\n",
    "    for i in range(imgs.shape[0]):\n",
    "        img = imgs[i]\n",
    "        img[img <= 0.5] = 0\n",
    "        img[img > 0.5] = 255\n",
    "        img = array_to_img(img)\n",
    "        img.save(pred_img_path+\"%s_pred.png\" %(name_list[i]))\n",
    "\n",
    "def predict_val(model,test_path,image_size=512):\n",
    "    test_list = glob.glob(test_path+'*.dcm')\n",
    "    data_pre(test_list)\n",
    "    imgs_test, imgs_label_test, test_name = create_test_data(test_path, image_size, image_size, 'test', 'jpg')\n",
    "    print('predict test data')\n",
    "    \n",
    "    imgs_label_pred = model.predict(imgs_test, batch_size=4, verbose=1)\n",
    "    name_list=test_name\n",
    "    df = pd.DataFrame(columns=['name', 'acc', 'sen', 'spe', 'dsc'])\n",
    "\n",
    "    true_list=imgs_label_test\n",
    "    print(true_list.shape)\n",
    "\n",
    "    pred_list=imgs_label_pred\n",
    "    print(np.unique(pred_list))\n",
    "    pred_list[pred_list > 0.5] = 1\n",
    "    pred_list[pred_list <= 0.5] = 0\n",
    "    \n",
    "    sensitivity=[]\n",
    "    specificity=[]\n",
    "    acc=[]\n",
    "    dsc=[]\n",
    "\n",
    "    for i in range(len(true_list)):\n",
    "        yt=true_list[i].flatten()\n",
    "        yp=pred_list[i].flatten()\n",
    "        mat=confusion_matrix(yt,yp)\n",
    "        if len(mat) == 2:\n",
    "            ac=(mat[1,1]+mat[0,0])/(mat[1,0]+mat[1,1]+mat[0,1]+mat[0,0])\n",
    "            st=mat[1,1]/(mat[1,0]+mat[1,1])\n",
    "            sp=mat[0,0]/(mat[0,1]+mat[0,0])\n",
    "            if mat[1,0]+mat[1,1] == 0:\n",
    "                specificity.append(sp)\n",
    "                acc.append(ac)\n",
    "            else:\n",
    "                sensitivity.append(st)  \n",
    "                specificity.append(sp)\n",
    "                acc.append(ac)\n",
    "        else:\n",
    "            specificity.append(1)\n",
    "            acc.append(1)\n",
    "\n",
    "        yt=true_list[i]\n",
    "        yp=pred_list[i]\n",
    "        if np.sum(yt) != 0 and np.sum(yp) != 0:\n",
    "            dice = np.sum(yp[yt==1])*2.0 / (np.sum(yt) + np.sum(yp))\n",
    "            dsc.append(dice)\n",
    "        df=  df.append({'name':name_list[i], 'acc':ac, 'sen':st, 'spe':sp, 'dsc':dice}, ignore_index=True)\n",
    "\n",
    "    print(\"complete\")      \n",
    "    print(\"acc avg : {0:0.4f}\".format(np.mean(acc)))\n",
    "    print(\"sensitivity avg : {0:0.4f}\".format(np.mean(sensitivity)))\n",
    "    print(\"specificity avg : {0:0.4f}\".format(np.mean(specificity)))\n",
    "    print(\"dsc avg : {0:0.4f}\".format(np.mean(dsc)))\n",
    "\n",
    "    predict_save(pred_list,name_list)\n",
    "    return test_name\n",
    "\n",
    "def get_results(test_name):\n",
    "    #test_name 순서대로\n",
    "    #4,5번 사이의 각도 측정해서 list로 저장\n",
    "    #그리고 4,5번 척추 사이의 거리 측정해서 list로 따로 저장\n",
    "    #부탁드립니다\n",
    "    angle_list = list(np.empty(shape=(60,), dtype=np.int8))\n",
    "    dist_list = list(np.empty(shape=(60,), dtype=np.int8))\n",
    "    return angle_list,dist_list #predict 결과들의 Angle 측정값\n",
    "\n",
    "\n",
    "def get_score(angle_list,dist_list,test_name,test_path):\n",
    "    angle_test = []\n",
    "    angle_ai = []\n",
    "    dist_test = []\n",
    "    dist_ai = []\n",
    "    get_no45 = []\n",
    "    for i in range(len(test_name)):\n",
    "        name = test_name[i]\n",
    "        data = []\n",
    "        \n",
    "        jsonfile = test_path+'/{}.json'.format(name)\n",
    "        for line in open(jsonfile,'r'):\n",
    "            data.append(json.loads(line))\n",
    "        for json_data in data:\n",
    "            check = 0\n",
    "            if json_data['annotation']['ANNOTATION_DATA'] is not None:\n",
    "                for m in json_data['annotation']['ANNOTATION_DATA']:\n",
    "                    if m['type']=='cobbAngle':\n",
    "                        if m['label'] == 'L4-5A':\n",
    "                            angle_test.append(m['angle'])\n",
    "                            angle_ai.append(angle_list[i])\n",
    "                            check = 1\n",
    "                    elif m['type']=='line':\n",
    "                        if m['label'] == 'L4-5H':\n",
    "                            dist_test.append(m['distMm'])\n",
    "                            dist_ai.append(dist_list[i])\n",
    "                            check = 1\n",
    "                if check==0:\n",
    "                    get_no45.append(name)\n",
    "    print(get_no45)\n",
    "    print(r2_score(angle_ai, angle_test))\n",
    "    print(r2_score(dist_ai, dist_test))\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Parser test input uid')\n",
    "    parser.add_argument('--train_path', type = str, default ='train/', help='학습데이터 위치')\n",
    "    parser.add_argument('--test_path', type = str, default ='test/', help='테스트데이터 위치')\n",
    "    parser.add_argument(\"--image_size\",type= int , default= 512, help='학습에 사용될 이미지의 크기')\n",
    "    parser.add_argument(\"--epochs\",type= int , default= 10, help='에폭')\n",
    "    parser.add_argument(\"--batch_size\",type= int , default= 4, help='배치사이즈')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    imgs_train, imgs_mask_train, imgs_name = train_data_loading(args.train_path, image_size = args.image_size)\n",
    "    model = deep(imgs_train,imgs_mask_train,args.train_path,batch_size = args.batch_size,epochs = args.epochs,image_size=args.image_size)\n",
    "    test_name = predict_val(model,args.test_path,image_size = args.image_size)\n",
    "    angle_list,dist_list = get_results(test_name)\n",
    "    get_score(angle_list,dist_list,test_name,args.test_path)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
